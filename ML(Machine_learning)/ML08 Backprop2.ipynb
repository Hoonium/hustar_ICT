{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning For Everyone and Robot\n",
    "[Lecture notes](https://github.com/idebtor/HuStar-ML) for HuStar Project by idebtor@gmail.com, Handong Global University\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 8 강: 역전파 2$^{Backpropagation}$ \n",
    "\n",
    "## 학습 목표\n",
    "    - 역전파의 원리를 이해한다\n",
    "\n",
    "## 학습 내용\n",
    "    - 출력층과 은닉층의 오차 구하기\n",
    "    - 비용 함수와 미분\n",
    "    - 역전파의 가중치 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 비용함수와 경사하강법\n",
    "\n",
    "다음 그림들은 기계학습 연구에 많이 사용하고 있는 세 종류의 붓꽃들을 관찰하여 그 중에 두 종류 붓꽃들을 퍼셉트론으로 분류한 결과를 시각화한 것입니다.   \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/iris3photo.png?raw=true\" width=\"400\">\n",
    "<center>그림 1: 기계 학습에서 자주 사용하는 3 종류의 붓꽃 </center>\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/iris-class2.png?raw=true\" width=\"400\">\n",
    "<center>그림 2: 두 종류의 붓꽃을 분류한 퍼셉트론 기계학습의 결과 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습의 결과를 관찰해보면, 판별식이 주어진 학습 자료들을 정확하게 분류를 하긴 하지만, 최적화된 것이라고 볼 수 없다는 것을 쉽게 알 수 있습니다.  두 종류의 붓꽃을 분류하는 판별식은 각 표본을 입력으로 받아 목표값과 비교하여 오류를 계산하고 그 결과를 가지고 가중치를 조정하는 것을 반복하는 방법으로 퍼셉트론을 이용하여 구한 것입니다.  표본들을 두 그룹으로 분류할 수 있는 최적화된 판별식을 구하는 방법이 아니라 두 그룹의 각 표본들을 분류할 수 있는 판별식을 구하는데 초점을 맞춘 알고리즘을 이용한 결과입니다.  \n",
    "\n",
    "이제 좀 더 거시적인 안목을 가지고 이 문제를 접근해 보길 원합니다. 그러면, 우리가 바라는 최적화된 판별식은 어떤 모양일까요?  \n",
    "\n",
    "그러한 판별식은 물론 두 그룹의 표본들 사이로 지나가면서, 아래 그림과 같이 각 표본으로부터 판별식까지 거리를 모두 합한 값을 가장 최소화할 수 있는 직선이 두 그룹을 분류하는 최적의 판별식이 될 것입니다.   \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/iris-class3.png?raw=true\" width=\"400\">\n",
    "<center>그림 3: 단순 분류를 위한 판별식(실선)과 최적화된 판별식(점선) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 구하고자 하는 판별식은 표본들에서 판별식까지 거리의 총합이 가장 작게 만드는 $x_2 = Wx_1 + b$와 같은 일차 방정식입니다.  여기서 $x_1, x_2$ 값은 여러 표본들로 이미 주어진 값들입니다.  각 표본들로부터 판별식까지의 거리를 최소화하는 W와 b를 구하면 됩니다. 이러한 함수를 비용 함수$^{cost \\ function}$ 혹은 손실 함수$^{loss \\ function}$ 라고 합니다.  우리가 일일이 다 계산할 수 없으므로, 컴퓨터를 이용하여 $W = 0.1, 0.2, 0.3, … , b = 0.1, … $값을 공식을 넣어보면서, 이 비용 함수가  가장 최소화되는 $W$와 $b$를 찾을 수도 있습니다.  이러한 접근법을 무차별적$^{brute \\ force}$방법이라고 합니다.   어떤 어려운 문제를 해결하기 위하여 무차별적 접근 방법으로 생각해내는 것도 이상한 것은 아니지만, 우리가 다루는 문제들에서 가중치 $W$와 편향 값의 변화와 그 조합은 헤아릴 수 없이 많아서 무차별적인 접근법은 실용적이지 않습니다.  \n",
    "\n",
    "이 문제는 오랫동안 수학자들을 괴롭혔고, 1960-70년이 되서야 실용적인 방법으로 해결될 수 있었습니다. 누가 먼저 주요 발견을 했는지에 대해서는 서로 다른 의견이 있지만 캐나다 힌튼 교수와 프랑스의 벤지오 교수의 논문은 기억할 만 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "힌튼 교수 는 2006년에 발표된 논문 을 통해서 신경망에 입력하는 초기값을 제대로 입력하면 여러 층에서도 연산이 가능하다는 것을 증명하였습니다. 곧 이어서 이듬해인 2007년에 캐나다의 벤지오 교수는 논문 에서 신경망이 충분히 깊으면 굉장히 복잡한 문제를 풀 수 있다는 것을 증명하였습니다. 이와 같이 새로운 이론과 알고리즘들이 복잡한 문제들을 뛰어나게 처리할 수 있는 현대 신경망에 대한 발전을 이끄는 계기가 되었습니다.\n",
    "\n",
    "아무튼 우리가 여기서 찾고자 하는 비용 함수를 최소화하는 $W$와 $b$를 구하려는 것이며, 이는 대수학을 이용하여 최소를 쉽게 찾을 수 없으며, 따라서 대안을 찾는 것인데 그 중에 하나의 방법이 __경사 하강법$^{gradient \\ descent}$__ 입니다. \n",
    "\n",
    "비용 함수가 어떤 복잡한 지형을 나타내는 지도이며, 우리는 깜깜한 밤에 자기 발 밑만 비추는 손전등을 가지고 있다고 가정합시다.  그런데, 우리가 전체 지형을 볼 수 있는 지도가 없지만, 그러나 우리는 가장 깊은 골짜기(비용 함수의 최저점)로 가기를 원한다는 것입니다.   우리가 가지고 있는 손전등으로 먼 곳을 볼 수 없으며, 현재 위치한 곳의 지형에서 어느 쪽이 아래 쪽으로 내려가는지는 볼 수 있다는 것입니다.  그러면, 우리는 아래쪽 방향으로 조금씩 움직일 것입니다.  조금씩 움직인 후 다시 그곳의 경사를 살펴보고 다시 아래 쪽으로 움직이면서 가장 아래쪽에 도착할 때까지 반복할 것입니다. 이렇게 하면, 우리는 전체 지형을 모르지만 목표를 향하여 점점 더 가까이 갈 수 있을 것입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수학적으로 생각을 해봅시다.  \n",
    "\n",
    "다음 그림은 간단한 함수 $E(w) = (w – 1)2 + 1$을 나타냅니다. 여기서 $E(w)$가 비용(우리의 경우는 오차) 함수라면, $E(w)$를 최소화하는 $w$를 찾고자 하는 것입니다.  경사 하강법을 사용한다면, 우리는 임의의 어느 한 점에서 출발해야 합니다. 그리고, 시작점에서 어느 쪽이 아래 쪽으로 가는 것인지 판단하고 그 방향으로 조금 움직여야 합니다.  \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescent2D.png?raw=true\" width=\"500\">\n",
    "<center>그림 4: 2차원 경사 하강법 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "그림에서 현 위치가 빨간 점이라면 $w$를 증가해야 하며, 초록 점이라면 $w$를 감소해야 합니다. 빨간 점이 위한 곳의 기울기는 음수이며, 초록 점이 있는 곳의 기울기는 양수인 것을 기억하십시오.  이렇게 조금씩 움직이면서 비용 함수의 최소값과 더 가까워지도록 우리의 위치를 개선하였다는 것을 알 수 있습니다.  최소값에 도달하거나 오차가 충분히 작을 때까지 이 학습 과정을 반복합니다.  양의 경사는 $w$을 감소시킨다는 것을 의미하고 음의 경사는 $w$을 증가시킨다는 것을 의미합니다. 이 그래프를 보면 명백하지만, 우리는 금방 잊어버리고 잘못하기 쉬우니 유의하십시오.\n",
    "\n",
    "또 한 가지 유의할 것은 최소값에 가까이 갈수록 움직이는 크기를 완화하지 않으면, 최소값을 지나칠 수 있습니다.  경사의 크기에 비례하여 움직이는 크기를 완화한다면, 우리가 가까이 갈 때 더 작은 크기로 움직이게 될 것입니다.  그림에서 초록 점들의 움직임이 이러한 개념을 설명하고 있습니다.  이러한 움직임은 대부분의 연속 함수에서는 전혀 나쁜 것이 아닙니다.  물론, 이러한 전략은 여기 저기 요철이 있는 이상한 함수 즉 불연속 함수에 대해서는 좋은 전략이 아닙니다.  \n",
    "\n",
    "이 경사 하강법을 사용할 때, 대수학을 사용하여 실제 최소값을 계산하지 않았습니다. 그 이유는 함수 $E(w) = (w-1)^2 + 1$이 아주 복잡하고 어려운 것이라고 가정했기 때문입니다. 수학적으로 기울기를 정확하게 계산하지 못했다고 하더라도 우리는 조금씩 올바른 방향으로 움직이면서 우리가 가고자 하는 곳으로 꽤 잘 찾아갈 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법은 많은 매개변수를 가진 함수를 사용할 때 더욱 빛납니다. $w$에 의해 결정되는 $E(w)$ 뿐만 아니라 $a, b, c, d, e, f$에 의해 결정되는 $E(w)$가 그러합니다. 신경망의 출력 함수, 즉 오차(비용) 함수는 아주 많은 가중치 매개변수에 의해 결정됩니다. 때로는 수 백 개가 될 때도 있지요!\n",
    "\n",
    "다음에서도 경사 하강법을 보여주지만 이번에는 2개의 매개변수를 사용하는 조금 더 복잡한 함수를 사용합니다. 이는 함수의 값을 나타내는 높이를 사용하여 3차원으로 표현할 수 있습니다.\n",
    "   \n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescent3D.png?raw=true\" width=\"400\">\n",
    "<center>그림 5: 3차원 경사 하강법 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러분은 아마 3차원 표면을 살펴보고 경사 하강법이 왼쪽에 보이는 다른 골짜기에서 끝나지는 않을지 궁금할 것입니다. 사실, 일반적으로 생각하면, 어떤 복잡한 함수는 많은 골짜기를 가지고 있을 것이기 때문에 경사 하강법이 가끔 옳지 않은 골짜기에 빠지지는 않을까요? 옳지 않은 골짜기는 무엇인가요? 가장 낮은 골짜기를 말합니다. 이에 대한 답은 그럴 수 있다는 것입니다.\n",
    "\n",
    "비용(오차) 함수의 최소 또는 틀린 골짜기에서 끝나는 것을 피하기 위해 언덕의 서로 다른 점에서 시작하여 신경망을 여러 번 학습시킵니다.  서로 다른 시작점은 서로 다른 시작 매개변수를 선택하는 것을 의미하며, 신경망의 경우에는 서로 다른 가중치를 처음에 선택하는 것을 의미합니다.  \n",
    "\n",
    "골짜기를 찾아 내려가던 발걸음을 잠시 멈추고 우리가 생각했던 것을 정리해봅시다.\n",
    "\n",
    "__요점:__\n",
    "\n",
    "- 경사 하강법은 함수의 최소값을 계산하는 매우 좋은 방법이며, 함수가 아주 복잡하고 대수학을 사용하여 수학적으로 쉽게 풀 수 없을 만큼 어려울 때 이 방법을 사용할 수 있다.\n",
    "- 더욱이, 많은 매개변수로 인해 다른 방법을 사용할 수 없거나 실용적이지 않을 경우 이 방법을 사용할 수 있다.\n",
    "- 또한, 이 방법은 데이터가 불완전할 때 탄력적$^{resilient}$ 이며, 함수가 아주 완벽하게 설명되지 않거나 가끔 잘못된 과정을 거칠 때 완전히 틀린 결과를 내지는 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 알아본 경사 하강법을 실제로 우리가 다루는 신경망에 적용해 봅시다. 퍼셉트론이나 신경망의 출력은 연결된 여러 가중치들이 출력에 영향을 주는 많은 매개변수이므로 이는 상당히 복잡하고 어려운 함수가 됩니다. 그러면 우리는 올바른 가중치를 계산하기 위해 경사 하강법을 사용할 수 있을까요?  네, 올바른 비용(오차) 함수를 선택한다면 그렇습니다.\n",
    "\n",
    "퍼셉트론이나 신경망의 출력 그 자체는 비용(오차) 함수가 아닙니다.  하지만 이것을 비용 함수로 바꿀 수 있습니다.  그 이유는 오차가 목표값$^{target}$ 즉 클래스 레이블과  신경망이 예측한 출력값과의 차이이기 때문입니다.   여기서 우리가 조심해야 할 것이 있습니다.  주어진 세 개의 출력에 대한 목표값과 출력값에 대하여 오차를 계산하는 세 가지 방법을 소개하고 있습니다. \n",
    "\n",
    "| 출력값     | 목표값 | 오차 계산법 1 | 오차 계산법 2 | 오차 계산법 3 |       \n",
    "|:----------:|:------:|:-------------:|:-------------:|:-------------:|\n",
    "| $\\hat{y}$  |   $y$  | $y-\\hat{y}$   | $|y-\\hat{y}|$ |$(y-\\hat{y})^2$|  \n",
    "|   0.4      |   0.5  |      0.1      |        0.1    |        0.01   |\n",
    "|   0.8      |   0.7  |     -0.1      |        0.1    |        0.01   |\n",
    "|   1.0      |   1.0  |      0.0      |        0.0    |        0.00   |\n",
    "|   Total    |        |      0.0      |        0.2    |        0.02   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비용 함수가 될 첫 번째 후보는 오차 계산법 1처럼 단순하게 (목표 – 출력) 즉 $y - \\hat{y}$이 됩니다. 이는 충분히 타당해 보입니다. 그렇죠? 글쎄요, 퍼셉트론이나 신경망이 얼마나 잘 학습되었는지 전체적인 상황을 알기 위해 노드 오차의 합을 살펴보게 되면, 합이 $0.0$이라는 것을 알 수 있습니다!\n",
    "\n",
    "무슨 일이 있었던 것일까요? \n",
    "\n",
    "퍼셉트론이나 신경망은 완벽하게 학습되지 않았습니다. 그 이유는 처음 두 개의 출력값이 목표값과 다르기 때문입니다. 합하여 0이 나온 것은 오차가 없다는 것을 의미합니다. 이러한 일이 일어나는 이유는 양의 오차와 음의 오차가 서로 상쇄되기 때문입니다. 완전히 상쇄 되지 않았더라도, 이는 오차를 잘못 측정한다는 것을 알 수 있습니다.\n",
    "\n",
    "이 문제를 오차 계산법 2처럼 오차의 절대값을 취해 이를 바로잡아 봅시다. 즉, 부호를 무시하고 $|y-\\hat{y}|$와 같이 할 수 있습니다. 이는 효과가 있을 수 있습니다. 왜냐하면 상쇄되는 것이 없을 것이기 때문입니다. 이 방법이 많이 사용되지 않는 이유는 기울기가 최소 값 근처에서 연속적이지 않으며, 이 비용 함수가 갖는 V형 골짜기 근처에서 튀어 오르기 때문에 이는 경사 하강법이 잘 동작하지 않도록 만들기 때문입니다. 이 경사는 최소 값과 가까워지도록 작아지지 않기 때문에, 단계가 작아지지 않으며, 지나칠 위험이 있습니다.\n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "    세 번째 선택 사항은 오차에 제곱 즉 $(y-\\hat{y})^2$을 하는 것입니다. 오차 계산법 3을 선호하는 이유에는 다음과 같이 여러 가지가 있습니다.</span>\n",
    "- 제곱 오차를 사용할 때 경사 하강법의 기울기를 계산하기 위한 대수학은 쉽습니다.\n",
    "- 비용 함수는 부드럽고 연속적이기 때문에(틈이나 급작스러운 변화가 없습니다) 경사 하강법이 잘 동작하도록 합니다. \n",
    "- 경사는 최소 값 근처에서 작아지며, 이를 움직이는 크기를 완화하기 위해 사용한다면 목표를 지나칠 위험은 작아진다는 것을 의미합니다.\n",
    "자, 이제 우리는 이제 마지막 단계에 와 있습니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법을 사용하기 위해, 우리는 가중치에 대한 비용 함수의 기울기를 계산해야 합니다.  이는 미적분학을 필요로 합니다. 더 정확하게는 미분이 필요합니다.  여러분은 이미 미분에 익숙할 수도 있습니다.  하지만 그렇지 않거나 복습이 필요하다면, 부록에 기초를 쉽게 설명한 것이 있으니, 참고하십시오.  미분은 어떤 것이 변화할 때 다른 어떤 것이 어떻게 변화하는지 계산하는 수학적으로 정확한 방법이라고 할 수 있습니다.  예를 들어 스프링을 늘리는 힘이 변화할 때 스프링의 길이는 어떻게 변화할까요?  여기서 우리는 신경망 내부의 연결된 가중치에 따라 비용 함수가 어떻게 변화하는지 알고자 합니다.  이를 다르게 \"가중치의 변화에 대해 오차는 얼마나 민감할까?라고 표현할 수 있습니다.\n",
    "\n",
    "그림은 우리가 달성하고자 하는 것에 집중할 수 있도록 해주기 때문에 그림을 보면서 시작해봅시다.  비용 함수의 $w$가 변화함에 따라 비용 즉 함수 $E(w)$값이 달라지므로 이를 상관 그래프로 그려보면 다음과 같은 형태를 이룰 것입니다.  \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient1.png?raw=true\" width=\"400\">\n",
    "<center>그림 6: 가중치 $w$와 비용함수 $E(w)$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그래프는 우리가 이전에 보았던 것과 아주 유사한 것입니다. 우리가 새로운 것을 하고 있는 것이 아닙니다.  이번에 최소화 하고자 하는 함수는 신경망의 오차 $E(w)$입니다. 우리가 개선하고자 하는 매개변수는 신경망의 가중치 $w$ 입니다. 예제에서는 하나의 가중치만을 보여주었지만 신경망은 더 많은 가중치를 갖고 있다는 것을 알고 있습니다.  가령, 오차(비용) 함수가 3차원 표면이면, 두 개의 가중치이며, 가중치가 많을수록 함수도 다양해 집니다. 3차원에서 오차를 최소화 하는 것은 여러 골짜기가 있는 산 지형과 비슷한 모양입니다.  함수의 매개변수가 많을수록 오차 표면을 시각화 하는 것은 점점 더 어려워집니다. 하지만 최소 값을 찾기 위해 경사 하강법을 사용한다는 개념은 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그림에서 $w$에 대하여 처음에 예측한 값 즉 초기 가중치를 위의 그림에서 흰색이라고 하면, 경사하강법은 현재 $w$의 위치에서, 경사가 아래로 되어 있는 부분으로 점을 움직이는 방법입니다.  어느 방향으로 $w$를 움직이면 비용이 작아지는지는 현재 $w$의 위치에서 비용 함수를 미분하면 됩니다.  이것을 수학적으로 써 보면 다음과 같습니다. \n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_j}} \n",
    "\\end{aligned}\n",
    "</span>\n",
    "\n",
    "이는 단순한 퍼셉트론의 경우에 가중치 $w_j$ 즉 변화할수록 오차 $E$가 얼마나 변화하는지 표현한 것입니다. 여러 노드가 존재하는 신경망의 경우, 하나의 노드 $j$와 다른 노드 $k$사이의 가중치에 대해서는 다음과 같이 $w_{ij}$로 표현할 수 있습니다.  \n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "\\end{aligned}\n",
    "</span> \n",
    "\n",
    "이 수식의 의미는 최소값을 향해 내려가고자 하는 오차(비용) 함수의 기울기$^{gradient}$입니다. 아래 그림에서 보는 바와 같이 이 기울기가 0가 될 때, 비용 함수는 가장 작은 값을 갖습니다.  <span style=\"color:blue\"> 비용 함수의 최소값을 찾는 알고리즘을 옵티마이저$^{optimiser}$라고 합니다.  많은 옵티마이저 알고리즘들 중에서 우리가 지금 다루고 있는 것은 경사 하강법$^{gradient \\ descent}$입니다. 이렇게 경사 하강법을 사용하는 옵티마이저를 선형 회귀$^{Linear \\ regression}$라고 부릅니다. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient2.png?raw=true\" width=\"300\">\n",
    "<center>그림 7: 비용함수 $E(w)$의 미분</center>\n",
    "\n",
    "이렇게 경사를 따라서 아래로 내려가다 보면 비용 함수가 최소화가 되는 $w$를 찾을 수 있습니다. 이렇게 경사 하강법$^{gradient \\ descent}$ 이란 이름처럼 경사(기울기)를 따라서 하강하면서(내려가면서) 최소값을 찾습니다.  다행히도 우리가 다루고 있는 제곱 오차의 합을 나타내는 함수 $E(w)$는 미분이 가능하고, 볼록한$^{convex}$함수입니다. 이러한 함수에는 경사하강법을 활용하여 함수 값을 최소로 하는 $w$를 찾을 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면, $W$와 $b$를 구할 때, $W$와 $b$를 어떻게 증가 또는 감소시켜서 비용 함수의 최소값을 가장 효과적으로 낼 수 있을까요?  이제 실제적으로 비용 함수를 만들고, 그 함수를 미분하여 비용 함수의 최소값을 구하는 수식을 도출하기도 합시다. \n",
    "경사 하강법을 사용하기 위해서는 비용 함수를, 학습 자료들의 목표값(혹은 측정값)과 기계학습에서 나온 출력값(혹은 예측값)의 제곱 오차의 합, __Sum of Squared Error(SSE)__ 혹은 평균 제곱 오차라는 함수를 사용합니다.  출력값과 목표값의 평균이나 절대값의 평균을 사용하지 않는 것에 유의하십시오.  제곱 오차의 합(SSE)을 표현하는 비용 함수 $E(w)$ 다음과 같은 수식으로 표현할 수 있습니다. \n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "\\begin{align} \n",
    "E(w) &= \\frac{1}{2} \\sum_i(y^{(i)} - \\hat{y}^{(i)})^2 \\\\\n",
    "     &= \\frac{1}{2} \\sum_i(y^{(i)} - \\phi(z^{(i)}))^2\n",
    "\\end{align} \n",
    "</blue>\n",
    "\n",
    "여기서  $y^{(i)}$는 $i$번째 표본의 클래스 레이블(목표값)이며 즉 우리가 이미 알고 있는 값이며, $\\hat{y}^{(i)}$은 그에 상응하는 예측값입니다. $\\phi(z^{(i)})$은 기계학습 모델의 출력값즉 예측값$^{predicted}$입니다. 시그마 연산자($\\sum$)는 모든 항목을 합한다는 것이며, $\\frac{1}{2}$은 차후에 함수를 미분을 할 때 편하게 하기 위해서 임의로 덧붙인 것인데, 비용 함수에는 아무런 영향을 미치지 않습니다. 여기서 $\\phi()$는 활성화 함수이며, 그 활성화 함수를 적용할 항은  $z = w^T x  = w_0 x_0 + w_1 x_1 + …+ w_n x_n$  입니다.   우리가 구하고자 하는 것은 비용 함수 $E(w)$ 값을 최소로 하는 가중치 $w$ 를 구하는 것입니다.  편향$^{bias}$ $b$ 값은 $b = w_0 x_0$이며, 단, $x_0 = 1$일 때라고 간주하여 처리합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 그러면 이제 $j$ 번째 가중치에 대해 SSE(제곱 오차의 합) 비용 함수 $E$를 $w_j$에 대하여 다음과 같이 미분을 계산할 수 있습니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_j}} &= \\frac{\\partial{}}{\\partial{w_j}}  \\frac{1}{2} \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right)^2 \\\\\n",
    "                                  &= \\frac{1}{2} 2 \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right) \n",
    "                                     \\frac{\\partial{}}{\\partial{w_j}} \\left(y^{(i)} - \\phi(z^{(i)})\\right) \\\\\n",
    "                                  &= \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right) \n",
    "                                     \\frac{\\partial{}}{\\partial{w_j}} \\left(y^{(i)} - \\sum_i(w_j^{(i)}x_j^{(i)})\\right) \\\\\n",
    "                                  &= \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right) (- x_j^{(i)}) \\\\\n",
    "                                  &= - \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right) x_j^{(i)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__미분 공식:__ \n",
    "아래 공식들에서 $u$와 $v$는 $x$에 관한 함수이고, $c$는 $x$에 관한 함수가 아닙니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{}}{\\partial{x}}(u^n) &= n u^{n-1}\\frac{\\partial{u}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{}}{\\partial{x}}(u + v) &= \\frac{\\partial{u}}{\\partial{x}} \n",
    "                                      + \\frac{\\partial{v}}{\\partial{x}}  \\\\\n",
    "\\frac{\\partial{}}{\\partial{x}}(uv) &= u\\frac{\\partial{v}}{\\partial{x}} \n",
    "                             + v\\frac{\\partial{u}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{}}{\\partial{x}}(cu) &= c\\frac{\\partial{u}}{\\partial{x}}, \\text{where c is not a function of x}   \n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러므로 조정해야 할 가중치 $\\Delta{W}$는 다음과 같이 나타낼 수 있습니다. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "W_{new}  &= W_{old} + \\Delta{W} \\\\\n",
    "         &= W_{old} + \\eta\\frac{\\partial{E}}{\\partial{w_j}}\n",
    "\\end{align}\n",
    "\n",
    "여기서 $\\eta$는 학습률은 나타냅니다. \n",
    "\n",
    "우리가 앞에서 계산 오차함수 미분값을 이용하여, 하나의 가중치 즉 $j$ 번째 가중치의 조정값 $\\delta w_j$에 대한 수식을 구하면 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "\\Delta{w_j}&= - \\eta \\frac{\\partial{E}}{\\partial{w_j}} \\\\\n",
    "           &= \\eta \\sum_i\\left(y^{(i)} - \\phi(z^{(i)})\\right) x_j^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "$\\sum_i()$는 모든 학습 자료를 사용하여 계산하여 $j$ 번째 가중치의 변화 즉 $\\Delta{w_j}$를 계산한다는 의미입니다. 가중치의 변화를 모두 계산한 후, 모든 가중치를 동시에 $\\Delta{w}$만큼 조정합니다. \n",
    "여기서 가중치 조정을 각 학습 자료를 학습한 후 매번 즉 점진적으로 가중치를 조정할 수 있고, 혹은 가중치 조정을 모든 학습 자료를 바탕으로 계산한 후, 한번에 가중치를 조정할 수도 있습니다.  전자의 방법을 확률적$^{stochastic}$ 경사하강법이라고 하며, 후자의 방법으로 가중치를 조정하는 알고리즘을 배치$^{batch}$경사하강법이라고 부릅니다. 배치$^{batch}$는  한 묶음으로 일괄 처리하는 방식을 의미하기 때문에 그러한 표현을 사용한 것입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 오차 함수의 미분\n",
    "\n",
    "우리는 이미 퍼셉트론의 가중치를 개선하는 것을 다루었지만, 아직 신경망의 가중치를 개선하는 데에 가장 중요한 질문에 대해 다루지 않았습니다. 우리는 이 문제를 다루기 위해 지금까지 작업을 하였으며, 거의 다 왔습니다. 이 질문을 해결하기 이전에 이해해야 할 한 가지 핵심 개념이 더 있습니다.\n",
    "\n",
    "지금까지, 신경망의 각 층으로 역전파되는 오차를 얻었습니다. 왜 그렇게 한 것일까요? 신경망에서 산출하는 출력(예측값)을 개선하기 위해 가중치를 어떻게 조정해야 하는지 알기 위해 오차를 사용했기 때문입니다. 이것은 기본적으로 강의 노트의 도입부분에서 선형 분류기로 다루던 것입니다.\n",
    "\n",
    "하지만 이 노드들은 단순한 선형 분류기가 아닙니다. 이렇게 더 정교한 노드는 노드로 들어가는 신호들을 합하여 순입력을 구한 후에, 시그모이드 활성화 함수를 적용합니다. 이러한 과정에 필수적인 가중치를 어떻게 조정할 수 있을까요? 왜 우리가 필요한 가중치를 대수학을 이용하여 직접 계산할 수 없을까요?\n",
    "\n",
    "가중치를 직접 계산하기 위해 대수학을 사용할 수 없는 이유는 수학이 너무 어렵기 때문입니다. 가중치의 조합이 너무 많아 신경망을 통해 신호를 앞으로 내보낼 때에 합해지는 함수가 너무 많습니다. 우리가 위에서 다루었던 것과 같이 각 층에 세개의 뉴런을 갖고 있는 삼층 신경망을 생각해보세요. \n",
    "\n",
    "첫 번째 입력 노드와 두 번째 은닉 노드 사이에 연결된 가중치를 개선하여 어떻게 세 번째 출력 노드의 출력을 $0.5$만큼 증가시킬 수 있을까요? 운이 좋다고 하더라도, 세 번째 출력 노드는 출력이 증가할지라도 다른 출력 노드까지 영향을 받아 다른 노드는 망가지게 될 것입니다. 이것이 간단하지 않다는 것을 여러분도 알 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__멋진 경사 하강법과 신경망 사이의 연결고리는 무엇일까요?__  \n",
    "\n",
    "글쎄요, 복잡하고 어려운 함수가 신경망의 오차라면, 최소를 찾기 위해 아래쪽으로 내려가는 것은 오차를 최소화하는 것을 의미합니다.  우리는 신경망의 출력을 개선하고 있습니다.  이것이 우리가 원하는 바입니다!  신경망의 출력은 연결된 가중치로 인해 출력에 영향을 주는 많은 매개변수를 갖기 때문에 복잡하고 어려운 함수입니다. 그러면 우리는 올바른 가중치를 계산하기 위해 경사 하강법을 사용할 수 있을까요? 네, 올바른 비용 함수를 선택한다면 그렇습니다.\n",
    "\n",
    "퍼셉프론의 경우와 마찬가지로 신경망의 출력 함수 그 자체는 비용 함수가 아닙니다.  하지만 이것을 비용 함수로 쉽게 바꿀 수 있다는 것을 알고 있습니다.  그 이유는 오차 즉 신경망의 출력값(예측값, $\\hat{y}$)과 클래스 레이블(목표값$y$)사이에 차이가 존재하기 때문입니다. \n",
    "\n",
    "여기서도 퍼셉트론과 마찬가지 이유로 목표값과 출력값의 차이에 제곱을 취한 것 즉 $(y - \\hat{y})^2$ 제곱 오차를 사용하여 경사 하강법에 사용할 비용 함수를 만듭니다.  그러면, 이제 우리가 원하는 것을 수학적으로 써봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "\\end{aligned}\n",
    "\n",
    "이는 가중치 $w_{jk}$가 변화할 때 오차 $E$가 얼마나 변화하는지 표현합니다. 이는 최소 값을 향해 아래로 내려가고자 하는 비용 함수의 기울기입니다.\n",
    "이 수식을 풀기 전에 잠시 동안 은닉층과 마지막 출력층 사이의 연결된 가중치에만 집중해봅시다. 다음 그림은 이것을 보여줍니다. 입력층과 은닉층 사이의 연결된 가중치에 대해서는 나중에 돌아와서 살펴봅시다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient_weight2.png?raw=true\" width=\"500\">\n",
    "<center>그림 8-1: 은닉층 노드 j와 출력층 노드 k의 사이의 가중치 $w^{[2]}_{jk}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient_weight3.png?raw=true\" width=\"400\">\n",
    "<center>그림 8-2: 은닉층 노드 $j$와 출력층 노드 $k$의 사이의 가중치 $w^{[2]}_{jk}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 미분을 사용하면서 각 기호가 무엇을 의미하는지 잊어버리지 않기 위해 이 그림을 계속 사용할 것입니다. 흥미를 잃지 마세요. 이는 어렵지 않으며, 모두 설명할 것이고 필요한 모든 개념은 이전에 모두 다루었던 것입니다.\n",
    "\n",
    "먼저, 신경망의 오차는 클래스 레이블($y_k$)과 출력값($\\hat{y}_k$)의 차이에 제곱을 취한 것을 모두 더한 비용 함수를 확장시켜 $n$개의 출력 노드를 합한 것입니다. 다음에서 식에서 살펴보길 바랍니다.\n",
    "\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} = \\frac{\\partial{}}{\\partial{w_{jk}}} \\sum_n (y _n - \\hat{y}_n)^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 우리가 한 것은 비용 함수 $E$가 실제로 무엇인지 적은 것입니다.\n",
    "\n",
    "노드 $n$개의 출력 $\\hat{y}_n$은 은닉층과 연결된 것에만 의존한다는 것을 인지하여 이를 단순화할 수 있습니다. 즉, 노드 $k$에 대해 출력 $\\hat{y}_k$은 가중치 $w_{jk}$에만 의존합니다. 그 이유는 이 가중치들이 노드 $k$로 들어가는 연결에 대한 것이기 때문입니다.\n",
    "다른 방법으로 이를 살펴보면 $b$와 $k$가 서로 같지 않을 때, 노드 $k$의 출력이 가중치 $w_{jb}$에는 의존하지 않습니다. 그 이유는 이를 이어주는 연결이 없기 때문입니다. 가중치 $w_{jb}$은 출력 노드 $k$가 아닌 $b$와의 연결에 대한 것입니다.\n",
    "\n",
    "이것이 의미하는 바는 우리가 합한 것에서 가중치 $w_{jk}$가 연결된 $\\hat{y}_k$을 제외한 모든 $\\hat{y}_n$을 제거할 수 있다는 것입니다. 이는 귀찮은 합을 모두 제거합니다! 이렇게 멋진 요령은 여러분이 익혀 두는 것이 좋을 것입니다.\n",
    "\n",
    "이것을 마쳤다면, 여러분은 처음에 비용 함수가 모든 출력 노드를 합하지 않아도 된다는 것을 발견했을 것입니다. 그 이유는 노드의 출력은 이어진 연결, 즉 해당 가중치에만 의존하기 때문이라는 것을 살펴보았습니다. 다른 책에서는 이 사실에 대해 얼버무리고 설명하지 않은 채로 비용 함수에 대해 언급합니다.\n",
    "\n",
    "어쨌든, 우리는 더 간단한 수식을 얻었습니다.\n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} = \\frac{\\partial{(y_k - \\hat{y}_k)^2}}{\\partial{w_{jk}}} \n",
    "\\end{aligned}\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미분을 조금 사용해봅시다. 여러분이 미분에 대해 익숙하지 않다면 부록을 참고할 수 있다는 것을 기억하세요.\n",
    "\n",
    "여기서 $w_{jk}$가 변할 때, 그와 아무 관계 없는 클래스 레이블 $y_k$는 당연히 변하지 않으므로 상수입니다. 즉 $y_k$은 $w_{jk}$의 함수가 아닙니다. 이에 대해 생각해보면, 가중치가 변함에 따라 클래스 레이블이 변한다면 정말 이상할 것입니다! 이렇게 해서 $w_{jk}$에 의존하는 $\\hat{y}_k$만을 남게 됩니다. 그 이유는 출력 $\\hat{y}_k$가 되는 신호를 보낼 때 가중치를 사용하기 때문입니다.\n",
    "\n",
    "우리는 미분의 연쇄 법칙$^{Chain \\ rule}$을 사용하여, 오차 함수의 미분을 감당할 수 있도록 아래와 같이 두 부분으로 나누어 봅니다.  \n",
    "\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} = \\frac{\\partial{E}}{\\partial{\\hat{y}_{k}}}  \\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 조금씩 해볼 수 있겠군요. 먼저, 제곱 함수에 도함수를 취하는 것은 쉽습니다. 이는 다음과 같습니다.\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "&= \\frac{\\partial{(y_k - \\hat{y}_k)^2}}{\\partial{\\hat{y}_{k}}} \n",
    "   \\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}} \\\\\n",
    "&= \\frac{\\partial{(y^2_k - 2y_k\\hat{y}_k +  \\hat{y}^2_k)}}{\\partial{\\hat{y}_{k}}} \n",
    "   \\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}}\n",
    "\\end{aligned}\n",
    "\n",
    "여기서 괄호 속의 첫 번째 항에 대한 도함수  $\\frac{\\partial{(y^2_k)}}{\\partial{\\hat{y}_{k}}}$에서, $y^2_k$은 $\\hat{y}_{k}$에 대하여 상수이므로 $0$가 됩니다. 두 번째 항에 대한 도함수 $\\frac{\\partial{(2y_k\\hat{y}_k})}{\\partial{\\hat{y}_{k}}}$에서, $2y_k$는 상수처럼 취급이 되고, $\\frac{\\partial{(\\hat{y}_k)}}{\\partial{\\hat{y}_{k}}}$ = 1이 되므로, 두 번째 항은 $2y_k$가 됩니다. 세 번째 항에 대한 도함수 $\\frac{\\partial{(\\hat{y}^2_k)}}{\\partial{\\hat{y}_{k}}} = 2 \\hat{y}_{k}$ 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned} \n",
    "&= (- 2y_k + 2 \\hat{y}_{k})\\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}} \\\\\n",
    "&= -2(y_k - \\hat{y}_{k})   \\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}}\n",
    "\\end{aligned}\n",
    "\n",
    "두 번째 항 $\\hat{y}_k$에 대해서는 조금 더 생각이 필요하지만 너무 과하지는 않을 것입니다. $\\hat{y}_k$는 출력층 노드 k의 출력이므로, 노드 $j$에 들어오는 입력 신호들에 각각의 가중치를 곱하여 합산한 값 즉 $k$ 노드의 순입력에 $k$노드의 활성화 함수 즉 시그모이드 함수를 적용한 것입니다. 이것을 모두 수식으로 정리하면, 다음과 같이 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> \n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "    &= -2(y_k - \\hat{y}_{k})   \\frac{{\\partial{\\hat{y}_{k}}}}{\\partial{w_{jk}}} \\\\\n",
    "    &= -2(y_k - \\hat{y}_{k})   \\frac{\\partial{}}{\\partial{w_{jk}}} g(z^{[2]}_k), \\text{  where an activation function $g()$, net input $z_k$} \\\\\n",
    "    &= -2(y_k - \\hat{y}_{k})   \\frac{{\\partial{}}}{\\partial{w_{jk}}}\n",
    "    sigmoid \\left({\\sum_j w_{jk}a^{[1]}_j}\\right), \\text{ where $g() = sigmoid()$}\n",
    "\\end{aligned}\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__위의 첫번째 식에서 $a^{[1]}_j$는 은닉층 노드의 출력입니다.__ \n",
    "\n",
    "두번째 식에서 $\\mathbf{\\frac{\\partial{}}{\\partial{w_{jk}}} g(z^{[2]}_k)}$은 출력층의 활성화 함수를 미분하여 순입력을 적용한 값을 오차와 곱셈을 하여 가중치 $w_{jk}$를 조정할 수 있다는 말입니다. \n",
    "\n",
    "세번째 식에서는 활성화 함수를 시그모이드로 정한 것이고, 또한 순입력의 계산 방법을 구체적으로 다시 표현한 것일뿐 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 사용한 시그모이드 함수를 계속 사용한다면, 어떻게 그 함수를 미분할 수 있을까요? 부록에 있는 기본적인 개념을 사용하여 해볼 수도 있지만, 우리는 전 세계의 수학자들이 매일 하는 것과 같이 이미 다음과 같이 알려진 답을 사용하면 됩니다.  \n",
    "\n",
    "다만, 여러분이 직접 도전해 볼지 못할 만큼 어렵지 않으니 한번쯤은 직접 시도 해보십시오.   \n",
    "\n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{}}{\\partial{x}}sigmoid(x) = sigmoid(x)(1 - sigmoid(x))\n",
    "\\end{aligned}\n",
    "\n",
    "어떤 함수는 미분을 했을 때 끔찍한 수식이 되기도 합니다. 이 시그모이드 함수는 간단하며, 결과를 사용하기 쉽습니다. 이는 신경망에서 활성화 함수로 시그모이드 함수를 널리 사용하는 이유 중 하나입니다. \n",
    "시그모이드 함수의 미분을 적용하면 다음과 같은 수식을 얻을 수 있으며, 마지막 항은 미분의 가장 기본적인 공식을 적용하여 계산한 항입니다. \n",
    "\\begin{align}\n",
    "\\frac{\\partial{}}{\\partial{x}}(u^n) &= n u^{n-1}\\frac{\\partial{u}}{\\partial{x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "&= -2(y_k - \\hat{y}_{k})   \\frac{{\\partial{}}}{\\partial{w_{jk}}}\n",
    "    \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) \\\\\n",
    "&= -2(y_k - \\hat{y}_{k}) \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right)  \n",
    "   \\left(1 - \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) \\right) \n",
    "   \\frac{{\\partial{}}}{\\partial{w_{jk}}} \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) \\\\\n",
    "&= -2(y_k - \\hat{y}_{k}) \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right)  \n",
    "   \\left(1 - \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) \\right) • a^{[1]}_j\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 항은 시그모이드를 미분하고, $sigmoid()$ 함수 안의 수식을 $w_{jk}$에 대해 미분해서 나온 것입니다.  이제 여기서 나온 마지막 항도 미분을 적용해야 하는데, 이것을 미분한 결과는 간단하게도 $ \\frac{{\\partial{}}}{\\partial{w_{jk}}} \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) = a^{[1]}_j$가 됩니다.  왜냐하면,  $w_{jk}$ 에 대하여 미분하므로  $w_{jk}$ = 1 이 되고, 결과는  $a^{[1]}_j$가 됩니다. \n",
    "\n",
    "마지막 정답을 기록하기 전에, 수식 앞에 있는 2를 제거해봅시다. 이렇게 할 수 있는 이유는 우리가 오차를 최소화할 수 있는 곳으로 내려가야 할 비용 함수의 기울기 방향에만 관심이 있기 때문입니다. 우리가 무엇에 집중해야 할지 알고 있기 때문에 이 수식 앞에 상수 요소 2, 3, 또는 100이 있을지라도 아무 상관 없습니다.\n",
    "\n",
    "이것은 우리가 지금까지 계산한 최종 정답이며, 이는 비용 함수의 기울기를 나타내며, 우리는 가중치 $w_{jk}$을 조정함으로 비용 함수의 기울기를 계산할 수 있습니다.  \n",
    "\n",
    "<span style=\"color:blue\"> \n",
    "\\begin{aligned} \n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    " &= -(y_k - \\hat{y}_{k}) \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right)  \n",
    "   \\left(1 - \\sigma \\left({\\sum_j w_{jk}•a^{[1]}_j}\\right) \\right) • a^{[1]}_j \\\\\n",
    "\\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    " &= -(y_k - \\hat{y}_{k}) \\sigma (z_k)(1 - \\sigma(z_k)) • a_j\n",
    "\\end{aligned}\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__휴! 해냈습니다!  축하합니다__\n",
    "\n",
    "이것이 우리가 찾던 수식입니다.  신경망을 학습시킬 열쇠이지요.  \n",
    "\n",
    "이것은 두 번 살펴볼 가치가 있습니다. \n",
    "\n",
    "첫 번째 괄호 부분 $(y_k - \\hat{y}_{k})$은 우리가 잘 알고 있는 오차입니다. \n",
    "\n",
    "시그모이드 함수 내부의 합 수식 $\\sum_j w_{jk}•a^{[1]}_j$은 마지막 층 노드로 들어가는 순입력이며, 간단히 표시하려면 $z_k$라 표시하는 것도 가능합니다. 이는 활성화 함수가 적용되기 전에 노드로 들어가는 신호 즉 순입력입니다.  \n",
    "\n",
    "마지막 부분에 $a^{[1]}_j$는 은닉층 노드 $j$에서 나오는 출력입니다. 이러한 호칭들에 대해서는 그림8-2를 참조하면 시각화하는데 도움이 될 것입니다. 이러한 수식은 유의하여 살펴볼 만한 가치가 있습니다. 이 기울기, 즉 가중치를 개선할 때에 무엇이 실제로 포함되어 있는지 느껴볼 수 있기 때문입니다.\n",
    "\n",
    "이것은 멋진 결과이며, 우리는 스스로 만족해야 합니다. 많은 사람들은 이 부분을 아주 어렵다고 느끼기 때문입니다. 여기까지 잘 해오신 여러분에게 박수를 보내드립니다. 다시 한번 축하합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 가중치 조정\n",
    "\n",
    "이제 기울기에 대해 모든 중요한 수식을 얻었기 때문에 이것을 이용하여 신경망의 모든 가중치를 업데이트할 수 있게 되었습니다.\n",
    "\n",
    "우리가 이전에 그림에서 분명히 보았던 것과 같이 가중치는 경사의 반대 방향으로 바뀌었다는 것을 기억하세요. 우리가 나쁜 학습 자료로 인해 너무 멀리 가는 것을 피하면서 동시에 가중치가 최소값을 계속 지나쳐 그 주위에서 튀지 않는 것을 보장하기 위한 방법으로 선형 분류기를 발전시킬 때에도 이것을 살펴볼 수 있었습니다. 이것을 수학적 형식으로 설명해봅시다.\n",
    "\n",
    "\\begin{aligned} \n",
    "new \\ w_{jk} = old \\ w_{jk} - \\eta \\frac{\\partial{E}}{\\partial{w_{jk}}} \n",
    "\\end{aligned}\n",
    "\n",
    "새로운 가중치 $new \\ w_{jk}$은 이전의 가중치 $old \\ w_{jk}$에 우리가 계산한 오차 기울기에 부정을 취하여 조정한 가중치입니다.  부정$^{negative}$을 취한 이유는 양의 기울기가 있을 때에 가중치를 감소시키고 음의 기울기가 있을 때에 증가시키기 위한 것입니다. 기호 에타$\\eta$는 최저치를 지나치지 않도록 하기 위해 변화의 정도를 완화시키는 요소입니다. 이를 학습률$^{learning \\ rate}$ 라고도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리 \n",
    "    - 인공 신경망의 신호 전달\n",
    "    - 인공 신경망의 신호 처리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
