{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning For Everyone and Robot\n",
    "[Lecture notes](https://github.com/idebtor/HuStar-ML) for HuStar Project by idebtor@gmail.com, Handong Global University\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 11 강: Gradient Descent 3 - Mini-Batch\n",
    "\n",
    "## 학습목표 \n",
    "- 경사하강법$^{gradient \\ descent}$ 학습의 정확도를 이해한다.\n",
    "- 배치$^{batch}$, 확률적$^{stochastic}$, 미니배치$^{mini-batch}$ 경사하강법들의 차이를 이해한다.\n",
    "- 다양한 기계 학습 기술을 익힌다. \n",
    "\n",
    "## 학습 내용\n",
    "- 다양한 경사하강법들의 장단점 비교하기\n",
    "- 배치$^{batch}$, 미니배치$^{mini-batch}$ 경사하강법$^{gradient descent}$의 정확도 이해하기 \n",
    "- 과적합$^{overfitting}$ 원인과 해결 방법\n",
    "- 학습 조기 종료$^{early \\ stopping}$\n",
    "- 데이터 증식\n",
    "- 드롭아웃$^{dropout}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 미니배치 경사하강법 신경망의 구현\n",
    "\n",
    "우리는 앞 강의에서 MNIST 데이터셋의 분류 문제를 다룰 수 있는 신경망에 대해 공부습니다.  이번에는 경사하강법을 좀 더 효율적으로 할 수 있는 확률적 경사하강법에 대해 알아 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 미니배치$^{mini-batch}$\n",
    "\n",
    "확률적 경사하강법은 자료를 하나씩 처리합니다. 미니배치 경사하강법은 자료를 하나씩 처리하는 것이 아니라 한 묶음씩 처리합니다. 미니배치란 훈련자료의 일부를 무작위로 선택한 자료의 한 묶음을 말합니다.  반복적으로 이런 미니배치에 대해서 경사하강법으로 가중치를 갱신하며 학습하는 것을 확률적 경사하강법이라고 부릅니다. \n",
    "\n",
    "MNIST 데이터셋은 60,000개의 훈련자료가 있습니다.  이 모든 자료를 대상으로 손실함수의 합을 구하려면 시간이 걸리기도 하고 메모리가 부족하기도 합니다.  더 나아가 빅데이터 문제 같은데에서는 문제가 더 심각해집니다. 이 많은 자료를 각각 손실함수를 계산하는 것은 현실적이지 않습니다. 이 경우 자료의 일부를 추려 전체의 근사치로 이용할 수 있습니다.  신경망 학습에서도 훈련자료의 일부만을 무작위로 선택하여 한 묶음으로 학습을 수행합니다. 한 묶음의 자료를 미니배치$^{mini-batch}라고 합니다. \n",
    "\n",
    "예를 들면, 60,000장의 훈련자료 중에서 100장을 무작위로 선택하여 한 묶음으로 학습을 진행하는 것입니다. 이러한 학습방법을 미니배치 학습이라고 합니다. \n",
    "\n",
    "MNIST 데이터셋의 훈련자료에서 자료를 100개 무작위로 뽑아내는 코드를 작성해봅시다.  제일 먼저 MNIST 데이터셋을 읽어오는 코드를 작성해봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xshape=(60000, 784), y.shape=(60000,)\n"
     ]
    }
   ],
   "source": [
    "import joy\n",
    "import numpy as np\n",
    "\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()    # reading mnist dataset\n",
    "print('Xshape={}, y.shape={}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드의 출력에서도 알 수 있듯이, 훈련자료의 이미지 형상과 레이블의 형상은 각각 (60000, 784), (60000,) 입니다.  테스트 자료의 이미지 형상과 레이블은 각각 (10000, 784), (10000, 1)입니다. 이러한 형상의 자료에서 우리는 무작위로, 예를 들면 100장을 선택하여 미니배치를 만들려고 한다면 어떻게 하면 될까요?\n",
    "\n",
    "넘파이의 `np.random.choice()` 함수를 사용하면 다음과 같이 간단히 해결할 수 있습니다.  `choice()`는 지정된 수의 범위 내에서 무작위로 원하는 갯수의 자료를 복사해올 수 있습니다.  예를 들면, `np.random.choice(60000, 5)`을 호출하면, 0 에서 60000미만의 수 안에서 무작위로 5개를 선택해줍니다. `random.choice()` 함수가 출력한 배열을 미니배치에서 뽑아낼 자료의 인덱스로 사용하는 것입니다. \n",
    "\n",
    "자, 그러면 `batch_size = 8`로 설정하여, 코드를 한 번 실제로 실행해 보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제\n",
    "MNIST 자료 훈련 샘플 `(X, y)` 가운데서 무작위로 `batch_size = 8` 즉 8개의 이미지를 택하여 `(Xbatch, ybatch)`를 생성하십시오. \n",
    "\n",
    "__A Sample Run:__\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient_batch1.png?raw=true\" width=\"600\">\n",
    "<center>그림 1: 배치 크기 8일 때의 한 경우(batch_size = 8) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mask= [58053 54661 52735 47458  6661  6690 24086 25172]\n",
      "ybatch= [0 6 7 6 9 6 1 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_samples = X.shape[0]\n",
    "batch_size = 8\n",
    "batch_mask = np.random.choice(m_samples, batch_size)\n",
    "xbatch = X[batch_mask]\n",
    "ybatch = y[batch_mask]\n",
    "print(\"batch_mask=\", batch_mask) # [ 5504 37702 11434 26189 57405 38151 11077 41116]\n",
    "print(\"ybatch=\", ybatch)         # [7 9 4 7 7 8 0 9]\n",
    "joy.show_mnist_grid(xbatch.reshape(-1, 28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치로 선택한 이미지 자료를 시각화해볼 수도 있습니다. 단, 유의할 것은 `load_mnist()`함수로 가져온 자료는 훈련을 위한 자료이므로 이미지로 시각화하기 위해서는 형상을 조정해야 합니다. 현재 `batch_size = m`이라고 한다면, xbatch의 형상은 `(m, 768)`인데, 이를 이미지로 표현할 수 있는 `(m, 28, 28)`형상으로 `reshape`해야 합니다.  reshape의 인자 중에 -1은 28x28형상을 중심으로 먼저 조정하고 나머지는 `reshape()`에서 조정하라는 의미입니다. -1을 사용하는 방법으로 `reshape` 인자를 코딩을 하면, `batch_size = 16`으로 변할지라도 시각화 함수를 호출하는 다음 코드는 변경하지 않아도 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 미니배치 신경망 훈련 \n",
    "\n",
    "미니배치를 이용한 신경망 훈련을 하기 전에, 우리가 바로 앞에서 구현하였던 `BatchGD` 클래스가 MNIST 데이터셋을 다루면서 \n",
    "\n",
    "다음 그림2은 `BatchGD`클래스의 입출력 자료의 형상과 변수, `fit()` 메소드가 호출하는 `feedpass()`메소드 부분의 연산을 보여주고 있습니다. `BatchGD`클래스에서는 훈련자료의 각 이미지로 신경망을 학습시키고 있다는 사실입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/MnistSGDOutline.png?raw=true\" width=\"600\">\n",
    "<center>그림 2: MiniBatchGD 신경망의 구현(배치크기, m = ..., 8, 16, 32, ... )</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 경사하강법은 확률적 경사하강법과 달리 `m = 1`이 아니라 `m = batch_size`로 만들어 신경망을 학습시키는 방법입니다. 중요한 것은 이미 `MiniBatchGD` 클래스의 fit 메소드에서 각 샘플을 하나씩 계산하지 않고 한 묶음씩 처리하는 것이며, 그 외에 다른 코드는 거의 같습니다.  미니배치에서 배치의 크기는 여러 옵션이 있겠지만, CPU/GPU 하드웨어와 메모리 구조를 생각해서 32의 배수 혹은 인수로 사용하는 것이 효과적이라고 합니다. 예를 들면, 8, 16, 32, 64, ... 등이 될 수 있을 것입니다.  \n",
    "\n",
    "배치처리는 계산 속도를 상당히 향상시킵니다.  이미지 한장당 처리 시간을 대폭 감소시켜줍니다. 그 이유는 어디에 있을까요? \n",
    "\n",
    "첫째는 파이썬이나 혹은 관련된 수치 계산 라이브러리가 대부분 큰 배열을 효율적으로 처리할 수 있도록 최적화 되어 있기 때문입니다.  \n",
    "둘째는 복잡한 신경망에서 속도에 대한 병목현상은 대부분 자료 전송에서 일어나는데, 배치형식으로 자료를 한번에 다량으로 (미니 배치로) 전달하는 방법은 그러한 자료 전송의 문제에 큰 도움이 됩니다. BUS(I/O)를 통해 자료를 읽는 속도보다 한번 많은 자료를 갖고 있어서 CPU/GPU의 연산 속도가 빨라질 수 밖에 없습니다.  작은 배열을 여러번 처리하는 것보다 많은 자료를 한번에 처리하는 것이 훨씬 빠르다는 것입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MiniBatchGD 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGD(object):\n",
    "    \"\"\" Mini-batch Gradient Descent \"\"\"\n",
    "    def __init__(self, n_x, n_h, n_y, eta = 0.1, epochs = 100, batch_size = 32, random_seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        #print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)          # hidden layer inputs\n",
    "        A1 = self.g(Z1)                      # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)          # output layer inputs\n",
    "        A2 = self.g(Z2)                       # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        X: input dataset \n",
    "        y: class labels\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)       # (m, n_y) = (m, 10)   one-hot encoding\n",
    "        #print('X.shape={}, y.shape={}, Y.shape={}'.format(X.shape, y.shape, Y.shape))\n",
    "        \n",
    "        for epoch in range(self.epochs + 1):\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "            error = 0\n",
    "            for i in range(0, m_samples, self.batch_size):\n",
    "                A0 = X[i: i + self.batch_size]\n",
    "                Y0 = Y[i: i + self.batch_size]\n",
    "                \n",
    "                #print(i + self.batch_size)\n",
    "                \n",
    "                A0 = np.array(A0, ndmin=2).T\n",
    "                Y0 = np.array(Y0, ndmin=2).T\n",
    "                \n",
    "\n",
    "                Z1, A1, Z2, A2 = self.forpass(A0)        \n",
    "\n",
    "                E2 = Y0 - A2                 \n",
    "                E1 = np.dot(self.W2.T, E2)       \n",
    "\n",
    "                # back prop, error prop\n",
    "                dZ2 = E2 * self.g_prime(Z2)     \n",
    "                dZ1 = E1 * self.g_prime(Z1)    \n",
    "\n",
    "                # update weights\n",
    "                self.W2 +=  self.eta * np.dot(dZ2, A1.T)     \n",
    "                self.W1 +=  self.eta * np.dot(dZ1, A0.T)   \n",
    "                error += np.sqrt(np.sum(E2 * E2))\n",
    "            self.cost_.append(error/self.batch_size)\n",
    "        testx = np.array(X[0:2],ndmin=2).T\n",
    "        testy = np.array(Y[0:2],ndmin=2).T\n",
    "        #print(testx)\n",
    "        #print(testy)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                         # activation_function: sigmoid\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "    \n",
    "    def g_prime(self, x):                   # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       \n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 MiniBatchGD fit() 메소드\n",
    "\n",
    "다음 코드는 `fit()`메소드에서 다른 경사하강법들과 다른 점은 미니배치를 아래와 같이 설정하는 부분입니다. \n",
    " \n",
    "```\n",
    "        for epoch in range(self.epochs + 1):\n",
    "            print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "            for i in range(0, m_samples, self.batch_size):\n",
    "                A0 = X[i : i + self.batch_size]\n",
    "                Y0 = Y[i : i + self.batch_size]    \n",
    "```\n",
    "`batch_size`는 사용자가 신경망 클래스 객체를 처음 생성할 때에 신경망의 속성으로 설정합니다. 디폴트로는 32로 초기화되어 있습니다.  매번 신경망을 학습시킬 때에 미니크기로 훈련자료를 차례대로 한 단위로 묶어 입력하여 기울기를 계산하고 오차를 계산하여 매개변수 가중치를 갱신하는 과정입니다. \n",
    "\n",
    "`for-loop`에서 주어진 훈련자료와 레이블 `m_samples`를 `batch_size`로 잘라서 차례대로 입력 `A0`, 출력 `Y0` 설정하여 연산을 진행합니다. 이것이 전부입니다. 정말 단순한 방법이지만 속도와 메모리 사용이 효율적이 됩니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MiniBatchGD MNIST 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0/10.\n",
      "MNIST self accuracy 99.63333333333333%\n"
     ]
    }
   ],
   "source": [
    "import joy\n",
    "import numpy as np\n",
    "\n",
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()   \n",
    "\n",
    "# To speed up a bit, just use the first 3000 images from 60000\n",
    "m = 3000\n",
    "X = X[:m]\n",
    "y = y[:m]\n",
    "\n",
    "# set hyperparameters and instantiate the class object       \n",
    "nn = MiniBatchGD(784, 100, 10, eta = 0.2, batch_size = 10, epochs = 10)  \n",
    "\n",
    "# train the model\n",
    "nn.fit(X, y)      \n",
    "\n",
    "# evaluate the accuray of itself\n",
    "accuracy = nn.evaluate(X, y)      \n",
    "print('MNIST self accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  MiniBatchGD MNIST 모델의 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist()   \n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST test accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 오차(self.cost_)의 시각화 \n",
    "\n",
    "신경망을 학습시키면서 발생하는 오차(손실)가 MnistMiniBatch 클래스 객체의 속성 `cost_`에 저장되어 있습니다. 이를 시각화해서 신경망이 어떻게 학습을 하였는지, 손실을 최소화하는 방향을 수렴하였는지 분석할 수 있습니다.  다음 셀의 코드를 실행해 봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.title('Mini-Batch GD: batch_size:{}'.format(nn.batch_size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overfitting: \n",
    "\n",
    "### 4.1 Increased Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist() \n",
    "\n",
    "X = X[:3000]\n",
    "y = y[:3000]\n",
    "epoch_list = [i for i in np.arange(5, 30, 5)] + [i for i in np.arange(50, 300, 30)]\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "for i, e in  enumerate(epoch_list):\n",
    "    nn = MiniBatchGD(784, 100, 10, epochs = e, batch_size = 64)  \n",
    "    nn.fit(X, y)  \n",
    "    self_accuracy.append(nn.evaluate(X, y))  \n",
    "    test_accuracy.append(nn.evaluate(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_list, self_accuracy, label='self')\n",
    "#plt.plot(epoch_list, test_accuracy, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting:Higher Epochs'.format(nn.batch_size))\n",
    "plt.legend(loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_list, self_accuracy, label='self')\n",
    "plt.plot(epoch_list, test_accuracy, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting:Higher Epochs'.format(nn.batch_size))\n",
    "plt.legend(loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Increased hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist() \n",
    "n_h_list = np.linspace(50, 800, 16, dtype=int)\n",
    "\n",
    "X = X[:3000]\n",
    "y = y[:3000]\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "for n_h in  n_h_list:\n",
    "    nn = MiniBatchGD(784, n_h, 10, epochs = 10, batch_size = 32)  \n",
    "    nn.fit(X, y)  \n",
    "    self_accuracy.append(nn.evaluate(X, y))  \n",
    "    test_accuracy.append(nn.evaluate(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_h_list, self_accuracy, label='self')\n",
    "plt.plot(n_h_list, test_accuracy, label='test')\n",
    "plt.xlabel('Number of Hidden Neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting:Higher Hidden Neurons')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MiniBatchGD_Dropout 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGD_Dropout(object): \n",
    "    \"\"\" Mini-batch Gradient Descent with Dropout \"\"\"\n",
    "    def __init__(self, n_x, n_h, n_y, eta=0.1, epochs=100, batch_size=32, random_seed=1, dropout_ratio=0.5):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(random_seed)\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        #print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0, train=True):\n",
    "        Z1 = np.dot(self.W1, A0)                # hidden layer inputs\n",
    "        A1 = self.g(Z1)                         # hidden layer outputs/activation func\n",
    "\n",
    "        # Dropout\n",
    "        if train:\n",
    "            self.drop_units = np.random.rand(*A1.shape) > self.dropout_ratio\n",
    "            A1 = A1 * self.drop_units / self.dropout_ratio\n",
    "          \n",
    "        Z2 = np.dot(self.W2, A1)                # output layer inputs\n",
    "        A2 = self.g(Z2)                         # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        X: input dataset \n",
    "        y: class labels\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)       # (m, n_y) = (m, 10)   one-hot encoding\n",
    "        #print('X.shape={}, y.shape={}, Y.shape={}'.format(X.shape, y.shape, Y.shape))\n",
    "        \n",
    "        for epoch in range(self.epochs + 1):\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "            error = 0\n",
    "            \n",
    "            for i in range(0, self.m_samples, self.batch_size):\n",
    "                A0 = X[i: i + self.batch_size].T\n",
    "                Y0 = Y[i: i + self.batch_size].T\n",
    "                Z1, A1, Z2, A2 = self.forpass(A0)        \n",
    "\n",
    "                E2 = Y0 - A2                 \n",
    "                E1 = np.dot(self.W2.T, E2)       \n",
    "\n",
    "                # back prop, error prop\n",
    "                dZ2 = E2 * self.g_prime(Z2)     \n",
    "                dZ1 = E1 * self.g_prime(Z1)    \n",
    "\n",
    "                # Dropout\n",
    "                dZ1 = dZ1 * self.drop_units       \n",
    "\n",
    "                # update weights\n",
    "                self.W2 +=  self.eta * np.dot(dZ2, A1.T)     \n",
    "                self.W1 +=  self.eta * np.dot(dZ1, A0.T)  \n",
    "                error += np.sqrt(np.sum(E2 * E2))\n",
    "            self.cost_.append(error/self.batch_size)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0, train=False)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                             # activation_function: sigmoid\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "    \n",
    "    def g_prime(self, x):                    # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       \n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MiniBatchGD_Dropout  MNIST모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "m = 3000\n",
    "X = X[:m]\n",
    "y = y[:m]\n",
    "nn = MiniBatchGD_Dropout(784, 600, 10, eta = 0.2, epochs = 10, dropout_ratio = 0.5)\n",
    "\n",
    "nn.fit(X, y)\n",
    "self = nn.evaluate(X, y)\n",
    "test = nn.evaluate(Xtest, ytest)\n",
    "print('MNIST self accuracy {}%'.format(self))\n",
    "print('MNIST test accuracy {}%'.format(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MiniBatchGD_Dropout  MNIST모델의 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum Error')\n",
    "plt.title('MiniBatchGD Dropout: batch_size:{}, ratio:{}'.format(nn.batch_size, nn.dropout_ratio))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam \n",
    "----------------------\n",
    "\n",
    "## Accuracy of Each Digit\n",
    "\n",
    "- get the histogram of each digit in the label `ytest`\n",
    "- get the yhat of test dataset\n",
    "- compare the output `yhat` and the label `ytest` to compute the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "import numpy as np\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()\n",
    "nn = MiniBatchGD(784, 200, 10, epochs = 10)\n",
    "nn.fit(X, y)\n",
    "self_accuracy = nn.evaluate(X, y)\n",
    "test_accuracy = nn.evaluate(Xtest, ytest)\n",
    "print('Test accuracy {}%'.format(np.round(test_accuracy, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo = np.bincount(ytest)\n",
    "print(histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of each digit\n",
    "A2 = nn.predict(Xtest)\n",
    "yhat = np.argmax(A2, axis = 0)\n",
    "truth = []\n",
    "for i in np.arange(10):\n",
    "    count = None (None for h, y in zip(yhat, ytest) if i == h == y)\n",
    "    truth.append(count)\n",
    "print('Digit Accuracy:', np.round(truth/histo, 2))\n",
    "print('Total Accuracy:', np.round(sum(truth)/sum(histo), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of each digit\n",
    "A2 = nn.predict(Xtest)\n",
    "yhat = np.argmax(A2, axis = 0)\n",
    "truth = []\n",
    "for i in np.arange(10):\n",
    "    count = sum(1 for h, y in zip(yhat, ytest) if i == h == y)\n",
    "    truth.append(count)\n",
    "print('Digit Accuracy:', np.round(truth/histo, 3))\n",
    "print('Total Accuracy:', np.round(sum(truth)/sum(histo), 3))\n",
    "\n",
    "\n",
    "print('Highest Accuracy:', np.argmax(truth/histo))\n",
    "print('Lowest  Accuracy:', np.argmin(truth/histo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute accuracy of each digit\n",
    "A2 = nn.predict(Xtest)\n",
    "yhat = np.argmax(A2, axis = 0)\n",
    "\n",
    "truth = []\n",
    "for i in np.arange(10):  # for each digit\n",
    "    count = 0\n",
    "    for hat, y in zip(yhat, ytest): \n",
    "        if i == hat == y: \n",
    "            count += 1\n",
    "    truth.append(count)\n",
    "\n",
    "print('Digit Accuracy:', np.round(truth/histo, 3))\n",
    "print('Total Accuracy:', np.round(sum(truth)/sum(histo), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "- 미니배치$^{Mini-Batch}$ 경사하강법을 학습하기\n",
    "- 학습 스케줄링\n",
    "- 과대적합의 원인과 해결 방법\n",
    "- 학습 조기 종료$^{early \\ stopping}$\n",
    "- 데디터 증식\n",
    "- 드롭아웃$^{dropout}$\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For God so loved the world that he gave his one and only Son, that whoever believes in him shall not perish but have eternal life. John3:16_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
