{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning For Everyone and Robot\n",
    "[Lecture notes](https://github.com/idebtor/HuStar-ML) for HuStar Project by idebtor@gmail.com, Handong Global University\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 11 강: Gradient Descent 1 - Batch\n",
    "\n",
    "\n",
    "## 학습목표 \n",
    "- 경사하강법$^{gradient \\ descent}$ 학습의 정확도를 이해한다.\n",
    "- 배치$^{batch}$, 확률적$^{stochastic}$, 미니배치$^{mini-batch}$ 경사하강법들의 차이를 이해한다.\n",
    "- 다양한 기계 학습 기술을 익힌다. \n",
    "\n",
    "## 학습 내용\n",
    "- 다양한 경사하강법들의 장단점 비교하기\n",
    "- 배치$^{batch}$, 미니배치$^{mini-batch}$ 경사하강법$^{gradient descent}$의 정확도 이해하기 \n",
    "- 과적합$^{overfitting}$ 원인과 해결 방법\n",
    "- 학습 조기 종료$^{early \\ stopping}$\n",
    "- 데이터 증식\n",
    "- 드롭아웃$^{dropout}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 경사하강법$^{Gradient \\ Descent}$\n",
    "\n",
    "우리는 앞 강의에서 MNIST 데이터셋의 분류 문제를 다룰 수 있는 신경망에 대해 공부했습니다. 또한, 신경망이 학습하는 가장 기본적인 (배치) 경사하강법에 대해 배웠습니다. 이번 장에서는 경사 하강법에 대해 복습하고, MNIST 데이터셋을 학습하는 실습을 하도록 하겠습니다.\n",
    "\n",
    "먼저 경사 하강법이란 현재 위치에서 어떤 함수의 극소점을 찾는 방법을 말합니다. 그렇다면 반대로 극대점을 찾는 방법은 경사 상승법이라고 합니다. 우리가 신경망을 학습 시킬 때에는 신경망의 예측 값과 레이블 간의 오차율을 작게 만드는 것이 목표이기 때문에 오차율의 극소점을 발견하는 경사 하강법을 사용합니다.\n",
    "\n",
    "다음과 같이 $x^2 + y^2$의 함수 그래프가 있다고 할 때, 그래프가 가장 움푹 파인 곳이 극소점이라는 것을 한 눈에 찾을 수 있을 것입니다.\n",
    "즉, 기울기가 0인 곳을 극소점/극대점 이라고 부르며 이는 그래프의 미분 값이 0인 곳을 말합니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescentMesh.png?raw=true\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 2차원 함수보다 차원이 높은 함수일 수록 발견하기 어려울 것 입니다. 신경망에서는 복잡한 함수의 극소점을 어떻게 찾아갈까요?\n",
    "\n",
    "아래 그림과 같이 여러분이 등산을 한다고 생각해 봅시다. 아니면, 숲이 우거져 앞이 보이지 않는 밀림에 있다고 상상해 봅시다. 여러분은 지도도 나침반도 없는 상황에서 정상에 도달하여야 합니다. 현재 있는 위치도 어디인지 모르고 숲이 우거져 한치 앞도 보이지 않습니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/GradientDescentEx.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 직관적인 방법으로는 현재 본인이 위치한 곳 보다 조금 더 높아 보이는 곳으로 가는 방법일 것입니다. 그리고 새로운 위치에서 다시 한번 지금 내가 갈 수 있는 가장 높아 보이는 곳으로 한 발 나아갑니다. 이런 작업을 반복하다 보면 결국 정상에 도달해 있지 않을까요?\n",
    "\n",
    "경사 하강법은 정상에 오르는 방법과 유사하게 오차율을 최소화 하는 극소점을 찾아 내려갑니다. 아래 식을 보며 조금 더 얘기해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\Delta w &= -\\eta \\Delta{J(w)} \\\\\n",
    "               &=-\\eta \\frac{\\partial{J(w)}}{\\partial{w_j}} \n",
    "\\end{align}\n",
    "\n",
    "우선 $\\eta$는 학습율을 의미하며, 위의 예제에서는 여러분들의 보폭을 말합니다. 한 발 내딛을 때 얼마나 큰 보폭으로 산을 올라갈지를 결정합니다. <br>또한, 우리가 원하는 것은 오차율의 극대점을 찾는 것의 반대이기 때문에 $\\eta\\ 앞에 -$기호를 사용하여 극소점을 거꾸로 찾아갑니다.<br>산의 정상에서 내려오는 일이라고 생각하면 됩니다. 뒤에 따라오는 미분 값은 신경망의 가중치에 따른 오차 값의 변화(기울기)가 0이 되는 점을 찾는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기본적인 경사 하강법에 대한 복습을 마쳤으니 MNIST 데이터셋을 직접 학습시켜보도록 하겠습니다. 아래는 앞에서 배운 비용함수 입니다.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2 \\tag1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 우리는 6만개의 MNIST 데이터 전체를 이용하여 학습하기 때문에 식(2) 처럼 데이터의 개수를 비용함수에 나누어 줘야 합니다.\n",
    "여기서는 데이터의 개수를 $m$으로 표기합니다.\n",
    "\n",
    "\\begin{align}\n",
    "J(w) = \\frac{1}{2m} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2  \\tag{2} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta w &= -\\eta \\Delta{J(w)} \\\\\n",
    "               &=-\\eta \\frac{\\partial{J(w)}}{\\partial{w_j}} \\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "최종적으로 신경망의 가중치는 6만개의 모든 데이터를 보고 그 중 가장 오차율을 최소화 하는 방향으로 $\\eta$만큼 한 걸음 내딛어 업데이트 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 배치 경사하강법 신경망의 구현 - BatchBD \n",
    "\n",
    "우리는 앞 강의에서 MNIST 데이터셋의 분류 문제를 다룰 수 있는 신경망에 대해 공부습니다. 주어진 학습 자료를 __모두 사용하여__ 오차를 계산한 후 그 오차를 반영하여 가중치를 조정하고 이것을 반복하며 최적의 가중치 값을 찾아가는 방법을  __배치$^{Batch}$ 경사하강법__ 이라고 합니다. 모든 학습 자료를 한 묶음$^{batch}$으로 처리하여 오차를 계산한다하여 그렇게 이름이 붙여진 모양입니다.  \n",
    "\n",
    "반면에 하나의 샘플이 신경망에 들어올 때마다 오차를 계산하고 가중치를 조정하는 방법을 __확률적$^{stochastic}$ 경사하강법__ 이라고 부릅니다. \n",
    "\n",
    "또한, 작은 묶음, 예를 들면, 32개의 자료를 묶음으로 하여 오차를 계산하고 가중치를 조정하는 방법을 __미니배치$^{mini-batch}$ 경사하강법__ 이라고 합니다. \n",
    "\n",
    "순전파와 역전파를 실행하면서 가중치를 조정하는 한 번의 과정을 __epoch__ 라고 합니다.  다음은 배치 경사하강법을 구현한 인공신경망 BatchGD_nn 클래스입니다. 이 클래스를 사용하여 MNIST 데이터셋을 학습하는 신경망 모델을 만들어낼 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/mnist_nn.py\n",
    "#%load code/mnist_nn.py\n",
    "import joy\n",
    "import numpy as np\n",
    "\n",
    "class BatchGD(object):\n",
    "    \"\"\" Batch Gradient Descent \"\"\"\n",
    "    def __init__(self, n_x, n_h, n_y, eta = 0.1, epochs = 100, random_seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        # print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)        # hidden layer inputs\n",
    "        A1 = self.g(Z1)                 # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)        # output layer inputs\n",
    "        A2 = self.g(Z2)                 # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y) \n",
    "        \n",
    "        for epoch in range(self.epochs + 1):\n",
    "            if epoch % 100 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "\n",
    "            A0 = np.array(X, ndmin=2).T       \n",
    "            Y0 = np.array(Y, ndmin=2).T     \n",
    "\n",
    "            Z1, A1, Z2, A2 = self.forpass(A0)  \n",
    "            E2 = Y0 - A2                      \n",
    "            E1 = np.dot(self.W2.T, E2)         \n",
    "\n",
    "            dZ2 = E2 * self.g_prime(Z2)          \n",
    "            dZ1 = E1 * self.g_prime(Z1)       \n",
    "            \n",
    "            self.W2 +=  self.eta * np.dot(dZ2, A1.T) / self.m_samples    \n",
    "            self.W1 +=  self.eta * np.dot(dZ1, A0.T) / self.m_samples    \n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        #x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):                    # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    \n",
    "    def evaluate_onebyone(self, Xtest, ytest):\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0\n",
    "        for m in range(m_samples):\n",
    "            A2 = nn.predict(Xtest[m])\n",
    "            yhat = np.argmax(A2)\n",
    "            if yhat == ytest[m]:\n",
    "                scores += 1        \n",
    "        return scores/m_samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST 인공신경망 모델 만들기\n",
    "\n",
    "다음 코드는 BatchGD 클래스와 MNIST 학습자료를 사용하여 MNIST 인공신경망 모델을 학습하고, 학습 자료 자체의 정확도를 측정합니다.\n",
    "\n",
    "__A Sample Run:__\n",
    "\n",
    "Samples:3000, hidden:100, eta:0.2, epochs:3000\n",
    "\n",
    "```\n",
    "Training epoch 0/3000.\n",
    "Training epoch 100/3000\n",
    "...\n",
    "Training epoch 2900/3000.\n",
    "Training epoch 3000/3000.\n",
    "MNIST self accuracy 98.05%\n",
    "```\n",
    "\n",
    "다음은 MINST 데이터셋의 첫 3000개의 이미지를 추출하여, 100개의 은닉층 노드 `n_h = 100`, 학습률 `eta = 0.2`, 반복 횟수 `epochs = 3000`으로 학습하는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape= (3000, 10)\n",
      "Training epoch 0/3000.\n",
      "Training epoch 100/3000.\n",
      "Training epoch 200/3000.\n",
      "Training epoch 300/3000.\n",
      "Training epoch 400/3000.\n",
      "Training epoch 500/3000.\n",
      "Training epoch 600/3000.\n",
      "Training epoch 700/3000.\n",
      "Training epoch 800/3000.\n",
      "Training epoch 900/3000.\n",
      "Training epoch 1000/3000.\n",
      "Training epoch 1100/3000.\n",
      "Training epoch 1200/3000.\n",
      "Training epoch 1300/3000.\n",
      "Training epoch 1400/3000.\n",
      "Training epoch 1500/3000.\n",
      "Training epoch 1600/3000.\n",
      "Training epoch 1700/3000.\n",
      "Training epoch 1800/3000.\n",
      "Training epoch 1900/3000.\n",
      "Training epoch 2000/3000.\n",
      "Training epoch 2100/3000.\n",
      "Training epoch 2200/3000.\n",
      "Training epoch 2300/3000.\n",
      "Training epoch 2400/3000.\n",
      "Training epoch 2500/3000.\n",
      "Training epoch 2600/3000.\n",
      "Training epoch 2700/3000.\n",
      "Training epoch 2800/3000.\n",
      "Training epoch 2900/3000.\n",
      "Training epoch 3000/3000.\n",
      "MNIST self accuracy 97.46666666666667%\n"
     ]
    }
   ],
   "source": [
    "import joy\n",
    "import numpy as np\n",
    "\n",
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()   \n",
    "\n",
    "# To speed up a bit, just use the first 3000 images from 60000\n",
    "\n",
    "# set hyperparameters and instantiate the class object       \n",
    "nn = BatchGD(None)  \n",
    "\n",
    "# train the model  \n",
    "\n",
    "# evaluate the accuray of itself\n",
    "accuracy = nn.None    \n",
    "print('MNIST self accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST 인공신경망 모델 적용하기\n",
    "\n",
    "MNIST 데이터셋을 위한 인공신경망의 모델을 만들었으므로, 이 모델을 만들 때 전혀 사용하지 않았던, 테스트 데이터셋을 이 모델에 적용하여 정확도를 측정하고자 합니다. \n",
    "\n",
    "### 3.1 Test 데이터 셋으로 정확도 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist()   \n",
    "accuracy = nn.evaluate(None, None)      \n",
    "print('MNIST test accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드는 Xtest, ytest자료의 처음 10개를 시각화하여 체크해보는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joy.show_mnist_grid(Xtest[:10].reshape(-1, 28, 28))\n",
    "print(ytest[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test 데이터 셋의 일부분으로 정확도 평가하기\n",
    "\n",
    "다음 코드는 Xtest, ytest 자료 10,000개 중에서 무작위를 1000개를 선택하여 테스트하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.choice(X.shape[0], 1000)\n",
    "Xtest = X[selected]\n",
    "ytest = y[selected]\n",
    "\n",
    "accuracy = nn.evaluate(Xtest, ytest)      \n",
    "print('MNIST test accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Epoch의 변화에 따른 정확도 평가하기\n",
    "다음 코드는 3000개의 샘플을 300번씩 epoch할 때마다 정확도를 측정하여 저장한 후, 그 변화를 시각화합니다. \n",
    "\n",
    "__A Sample Run:__ \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient_batch2.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist() \n",
    "# To speed up a bit, just use the first 3000 images from 60000\n",
    "\n",
    "epoch_list = [i for i in np.arange(300, 2100, 300)]\n",
    "print(epoch_list)\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "for i, e in  enumerate(epoch_list):  \n",
    "    nn = BatchGD(784, 100, 10, eta=0.2, epochs = e)  \n",
    "    nn.None\n",
    "    self_accuracy.append(None)  \n",
    "    test_accuracy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epoch_list, self_accuracy, label='self')\n",
    "plt.plot(epoch_list, test_accuracy, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Samples:{}, n_h:{} eta:{}, epochs:{}\".format(len(y), nn.n_h, nn.eta, nn.epochs))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 오차(self.cost_)의 시각화 \n",
    "\n",
    "신경망을 학습시키면서 발생하는 오차(손실)를 BatchGD 클래스 객체의 속성 `cost_`에 저장되어 있습니다. 이를 시각화해서 신경망이 어떻게 학습을 하였는지, 손실을 최소화하는 방향을 수렴하였는지 분석할 수 있습니다.  다음 셀의 코드를 실행한 후, 그 결과를 아래와 같이 시각화할 수 있습니다.  \n",
    "\n",
    "__A Sample Run:__ \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/gradient_batch3.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mnist dataset\n",
    "(X, y), (Xtest, ytest) = joy.load_mnist()   \n",
    "X = X[:1000]\n",
    "y = y[:1000]   \n",
    "nn = BatchGD(784, 100, 10, eta = 0.2, epochs = 300)  \n",
    "nn.fit(X, y)      \n",
    "accuracy = nn.evaluate(X, y)      \n",
    "print('MNIST self accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Squared Sum')\n",
    "plt.title(\"Samples:{}, n_h:{} eta:{}, epochs:{}\".format(len(y), nn.n_h, nn.eta, nn.epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 테스팅을 위한 질문들...\n",
    "\n",
    "- 과적합$^{overfitting}$이 과연 언제 일어날 것인가? \n",
    "    - 고의로 과적합이 좀 쉽게 일어나게 하려면 어떻게 조건으로 모델을 만들면 될 것인가?\n",
    "- 삼층 신경망으로 제한된 모델에서, 어떤 조건으로 어느 정도까지 정확도를 높일 수 있을 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (Xtest, ytest) = joy.load_mnist() \n",
    "\n",
    "m = 1000          # number of samples\n",
    "X = X[:m]\n",
    "y = y[:m]\n",
    "epoch_list = [i for i in np.arange(1000, 5000, 500)]\n",
    "print(epoch_list)\n",
    "self_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i, e in  enumerate(epoch_list):\n",
    "    nn = BatchGD(784, 150, 10, eta=0.2, epochs = e)  \n",
    "    nn.fit(X, y)  \n",
    "    self_accuracy.append(nn.evaluate(X, y))  \n",
    "    test_accuracy.append(nn.evaluate(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epoch_list, self_accuracy, label='self')\n",
    "plt.plot(epoch_list, test_accuracy, label='test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Early Stopping:\"Beautiful free lunch\"')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "_For God so loved the world that he gave his one and only Son, that whoever believes in him shall not perish but have eternal life. John3:16_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
