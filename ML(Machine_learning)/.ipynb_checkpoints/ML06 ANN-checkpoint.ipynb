{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning For Everyone\n",
    "Lecture notes for HuStar Project by idebtor@gmail.com \n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 5 강: 인공 신경망$^{Artificial \\ Neural \\ Network}$ \n",
    "\n",
    "## 학습 목표\n",
    "    - 인공 신경망 신호전달의 원리와 처리 방법을 이해한다. \n",
    "\n",
    "## 학습 내용\n",
    "    - 인공 신경망 신호 전달의 원리\n",
    "    - 인공 신경망 신호 처리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 인공 신경망의 신호전달 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전기 신호는 수상돌기에 의해 모이게 되고 더 강한 전기 신호를 형성합니다. 이 신호가 임계값을 지나기에 충분히 강하다면, 뉴런은 축색돌기를 지나 신경 말단으로 신호를 발사하여 다음 뉴런의 수상돌기로 전달합니다. 다음 그림은 이렇게 연결된 여러 뉴런을 보여줍니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/in search of memory_big.jpg?raw=true\" width=\"400\">\n",
    "<center>그림 1: 뉴론과 신경망 </center>\n",
    "<center>출처:[Australian Museum](https://australianmuseum.net.au/image/in-search-of-memory-eureka-prizes), 2015, Victor Anggono </center>\n",
    "\n",
    "뉴론들이 서로 연결된 망을 신경망$^{neural \\ network}$이라고 합니다. 생물학적 뇌의 기본 단위인 뉴론들이 망$^{network}$으로 연결되어 서로 신호를 전달하면서 필요한 연산을 합니다. 뉴론을 서로 연결하는 시냅스의 수는 100조나 된다고 합니다. 그러니까 요즘 아무리 컴퓨터 기능이 뛰어나도, 병렬로 연결할지라도, 인간 한 사람의 두뇌에서 일어나는 신경망의 연산을 따라갈 수 없는 것입니다.\n",
    "\n",
    "우리가 알아야 할 것은 발사될 때에, 각 뉴런이 그 전에 있던 여러 뉴런으로부터 입력을 받아 더 많은 뉴런으로 신호를 전달한다는 것입니다.\n",
    "이러한 자연적인 것을 인공 모델로 모방할 수 있는 한 가지 방법은 이전 층layer과 다음 층의 뉴런을 서로 연결하여 뉴런 층을 만드는 것입니다. 다음 그림은 이 개념을 설명합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_model.png?raw=true\" width=\"400\">\n",
    "<center>그림 2: 생물학적 신경망을 인공 신경망으로 모델링</center>\n",
    "\n",
    "여러분은 각 층에 있는 세 개의 인공 뉴런들, 또는 노드node들이 세 층에 있는 다른 뉴론들과 서로 연결되어 신경망을 이루는 것을 볼 수 있습니다. 또한 각 노드의 이전 층과 다음 층에 다른 노드와 연결되어 있습니다.\n",
    "\n",
    "멋집니다! 하지만 이렇게 멋진 구조에서 어떤 부분이 학습을 하게 될까요? 학습 자료에 기반하여 무엇을 조정해야 할까요? 우리가 이전에 살펴보았던 선형 분류기의 기울기와 같이 우리가 개선해야 할 매개변수가 있을까요?\n",
    "\n",
    "가장 확실한 것은 노드 사이의 연결 강도를 조정하는 것입니다. 노드 사이에 입력 합을 조정할 수도 있고, 시그모이드 임계 함수의 모양을 조정할 수도 있을 것입니다. 하지만 단순히 노드 사이의 연결 강도를 조정하는 것보다는 복잡합니다.\n",
    "\n",
    "간단한 방법이 있다면, 그것을 사용해봅시다!  다음  그림은 연결된 노드와 함께 각 노드 연결에 대한 가중치weight를 보여줍니다. 낮은 가중치는 신호를 약화시키고, 높은 가중치는 더 증폭시킬 것입니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_model2.png?raw=true\" width=\"400\">\n",
    "<center>그림 3: 인공 신경망의 가중치 표기법 (가중치를 다 표시한 것은 아님)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 기호 옆에 적혀 있는 작은 숫자에 대해 설명하는 것이 필요하겠군요. 가중치 w 옆에 2,3은 한 층의 노드 2에서 다음 층의 노드 3 사이에 전달되는 신호를 뜻합니다. 따라서 w1,2은 노드 1와 다음 층의 노드 2 사이의 신호를 약화시키거나 증폭시키는 가중치입니다. 이 개념을 설명하기 위해, 다음 그림에서는 첫 번째 층과 두 번째 층 사이의 두 연결을 보여줍니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/3layer_model3.png?raw=true\" width=\"400\">\n",
    "<center>그림 4: 입력층 1의 노드과 은닉층 2의 노드의 연결과 가중치 예시 </center>\n",
    "\n",
    "여러분은 이 설계에 대하여 왜 각 노드가 이전 층과 다음 층의 다른 모든 노드와 연결되어야 하는지 궁금해할 것입니다. 꼭 그렇지 않아도 되며, 여러분은 여러가지 창의적인 방법으로 연결할 수 있습니다. 우리가 이렇게 하지 않은 이유는 균일한 연결이 컴퓨터 명령으로 암호화 하는데 더 쉽고, 특정 일을 해결하는 데 필요한 최소 연결보다는 더 많은 연결이 있는 것이 큰 손해는 없을 것이기 때문입니다. 학습 과정에서 여분의 연결이 실제로 필요하지 않다면 그 연결에 중점을 두지는 않을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "여기서 무엇을 의미하는 것일까요? 이 네트워크가 내부의 연결된 가중치들을 바꾸면서 출력을 개선하기 위해 학습할 때, 어떤 가중치는 0이 되거나 0과 가까워집니다. 0, 또는 거의 0인 가중치는 신호가 통과할 수 없기 때문에 이 연결이 네트워크에 기여하지 못한다는 것을 뜻합니다. 0인 가중치는 신호에 0을 곱하기 때문에 그 결과는  0이 되어 연결이 끊어집니다.\n",
    "\n",
    "요점:\n",
    "\n",
    "- 생물학적 뇌는 현대 컴퓨터보다 적은 저장 공간을 가지고 훨씬 느리게 동작하는 것처럼 보임에도 불구하고 비행, 먹이 찾기, 언어 학습, 포식자 피하기 등과 같은 정교한 일을 수행합니다.\n",
    "- 또한, 생물학적 뇌는 전통적인 컴퓨터 시스템에 비해 손상과 불완전한 신호에 대한 회복력이 좋습니다.\n",
    "- 연결된 뉴런으로 이루어진 생물학적 뇌는 인공 신경망의 영감이 되었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 인공 신경망의 신호처리 \n",
    "\n",
    "세 층$^{three layers}$으로 이루어진 뉴런들이 다음 층이나 이전 층의 다른 모든 뉴런들과 연결되어 있는 것이 복잡해 보입니다.  하지만 신호가 어떻게 입력에서 층들에 여러 뉴런(노드)을 지나 출력이 되는지 또한 그 출력을 계산한다는 것은 벅차고 너무 어려워 보입니다!\n",
    "\n",
    "이것을 다 이해하려면 단 시간에는 어려울 것입니다. 그래서, 나중에는 컴퓨터를 사용하여 우리의 모든 일을 처리한다 하더라도 우선적으로 신경망 내부에서 실제로 어떤 일이 일어나는지에 대해 먼저 이해하는 것이 필요합니다. 간단히 시작해보죠.\n",
    "\n",
    "아래와 같이 각각 2개의 뉴런을 갖는 2개의 층으로만 이루어진 작은 신경망을 이용해 작업해봅시다. 이 작은 인공 신경망의 두 개의 입력이 각각 1.0와 0.5라고 가정해봅시다. 다음은 신경망에 들어가는 두 개의 입력을 보여줍니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model.png?raw=true\" width=\"400\">\n",
    "<center>그림 5: 각각 2개의 뉴런을 갖는 2개층의 인공 신경망과 두 개의 입력 </center>\n",
    "\n",
    "이와 같이, 각 노드는 활성화 함수를 순입력에 적용하여 노드의 출력으로 바꿉니다.\n",
    "이 때, 활성화 함수로 시그모이드 함수 $\\frac{1}{1+e^{-z}}$을 사용합니다. 여기서 `z`는 순입력이며, 시그모이드를 적용한 `z`는 뉴런의 출력 `y`가 됩니다.   \n",
    "\n",
    "그러면, 가중치는 어떻게 될까요? 가중치는 어떤 값으로 시작해야 할까요? 이는 아주 좋은 질문입니다. 임의의 가중치를 사용해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\\begin{equation}\n",
    "w_{1,1} = 0.9 \\\\\n",
    "w_{1,2} = 0.2 \\\\\n",
    "w_{2,1} = 0.3 \\\\\n",
    "w_{2,2} = 0.8 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 값에서부터 시작하는 것은 아주 나쁜 방법이 아니며, 이는 우리가 이전에 다루었던 단순 선형 분류기의 처음 기울기 값을 선택하기 위해 사용했던 방법입니다. 분류기가 학습했던 각 표본으로부터 임의의 값이 개선되었습니다. 신경망의 가중치들에도 똑같이 적용됩니다.\n",
    "\n",
    "이 작은 신경망에는 각 층의 노드 2개씩을 연결하기 위한 조합으로 가중치가 4개만 존재합니다. 다음 그림은 모든 숫자가 표시된 것을 보여줍니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model2.png?raw=true\" width=\"400\">\n",
    "<center>그림 6: 인공 신경망의 입력과 가중치 </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산을 시작해봅시다!\n",
    "\n",
    "첫 번째 노드 층은 입력층이며, 입력 신호를 표시하는 것 외에는 특별히 하는 일이 없습니다. 즉, 입력 노드에서는 입력에 활성화 함수를 적용하지 않습니다. 지금까지 그래왔기 때문에 여기에는 특별한 이유는 없습니다. 신경망의 첫 번째 층은 입력층이며, 모든 첫 번째 층은 입력을 표시합니다.\n",
    "첫 번째 입력층 1에서는 계산할 것이 없기 때문에 쉽습니다.\n",
    "\n",
    "다음은 두 번째 층이며, 여기에서는 약간의 계산이 필요합니다. 두 번째 층의 각 노드에서는 결합한 입력을 계산해야 합니다. 시그모이드 함수 $h(x)=1/(1+e^{-x})$에 __순입력__$^{net \\ input} z$를 적용하기 위해 $x$대신 $z$를 적용합니다. 이것은 이전 층의 연결된 노드들로부터 나오는 출력을 합한 것이지만, 연결된 가중치들에 의해 조정(증폭 혹은 완화)됩니다. 다음 그림은 이전에 보았던 것과 비슷하지만 이번에는 들어오는 신호를 연결된 가중치로 완화시켰습니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model3.png?raw=true\" width=\"400\">\n",
    "<center>그림 7: 인공 신경망의 출력 계산하기 </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 층의 첫 번째 노드를 위의 그림에서 확대해서 그려 놓았는데, 첫 번째 입력층의 모든 노드는 이 노드와 연결되어 있습니다.  이 입력 노드의 값은 `1.0`와 `0.5`입니다. 첫 번째 노드로부터 연결될 때에는 가중치가 `0.9`이며, 두 번째 노드로부터 연결될 때에는 가중치가 `0.3`입니다. 따라서 조정된 입력 값들의 합 즉 순입력은 다음과 같습니다.\n",
    "\n",
    "$\n",
    "\\mathbf{z}=(첫 번째 노드의 출력* 연결된 가중치)+(두 번째 노드의 출력* 연결된 가중치) \\\\\n",
    "\\mathbf{z}=(1.0*0.9)+(0.5*0.3) = 0.9 + 0.15 = 1.05 \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신호를 완화하지 않았다면, 간단하게 신호를 더하여 `1.0 + 0.5`가 되었겠지만 우리는 이것을 원하지 않습니다. 가중치는 더 나은 결과를 얻기 위해 반복적으로 개선하면서 신경망에서 학습을 하기 때문입니다.\n",
    "\n",
    "따라서 두 번째 층 첫 번째 노드의 조정된 입력으로 우리는 `x = 1.05`을 얻었습니다. 마지막으로, 활성화 함수 $y=1/(1+e^{-x})$을 사용하여 노드의 출력을 계산할 수 있습니다. 여기에서 계산기를 사용해도 됩니다. 정답은 $y=1/(1+0.3499) =1/1.3499$입니다. 따라서 `y1 = 0.7408`이 됩니다.\n",
    "\n",
    "좋습니다! \n",
    "\n",
    "우리는 이제 신경망의 두 출력 노드 중 하나의 출력을 얻었습니다.\n",
    "이제 나머지 노드, 즉 두 번째 층의 두 번째 노드로 다시 계산해봅시다. 입력 x를 조정한 것을 더하면 다음과 같습니다.\n",
    "\n",
    "$\n",
    "\\mathbf{z}=(첫 번째 노드의 출력* 연결된 가중치)+(두 번째 노드의 출력* 연결된 가중치) \\\\\n",
    "\\mathbf{z}=(1.0*0.2)+(0.5*0.8) = 0.2 + 0.4 = 0.6\n",
    "$\n",
    "\n",
    "이제 `x`값이 주어졌기 때문에, 시그모이드 활성화 함수를 사용하여 노드의 출력을 계산할 수 있습니다. 계산하면 `y = 1/(1+0.5488) = 1/(1.5488)`이 됩니다. 따라서 `y2 = 0.6457`입니다.   이 작은 신경망을 `Jupyter notebook`으로 구현하면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05 0.6 ]\n",
      "[0.7407749  0.64565631]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "x = np.array([1.0, 0.5])\n",
    "w = np.array([[0.9, 0.3], [0.2, 0.8]])\n",
    "z = np.dot(w, x)\n",
    "print(z)\n",
    "y = sigmoid(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 그림에서는 우리가 방금 계산한 신경망의 출력을 보여줍니다. 단순화한 신경망에서 두 출력을 얻은 것은 계산할 만 했습니다. 더 큰 신경망을 이렇게 직접 계산하는 것은 원하지 않겠죠?  다행스럽게도 컴퓨터는 이런 종류의 계산을 전혀 지치지 않고 정말 빠르고 정확하게 계산하는 데 아주 완벽합니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model4.png?raw=true\" width=\"400\">\n",
    "<center>그림 8: 인공 신경망의 출력 </center>\n",
    "\n",
    "그렇다 하더라도 2개 이상의 층을 갖고, 각 층에 4개, 8개, 또는 100개의 노드를 가진 신경망을 계산하는 컴퓨터 명령을 작성하고 싶지는 않습니다. 명령을 작성하는 것만 해도 지칠 것이며, 모든 노드와 모든 층에 대한 명령을 작성할 때에 실수를 할 수도 있기 때문에 실제로 계산을 하는데 다분히 실수할 가능성이 매우 놓겠죠?\n",
    "\n",
    "다행히, 신경망이 더 많은 층과 노드를 갖고 있더라도 출력을 계산하는 데 필요한 연산을 작성할 때에 수학을 이용하면 더 간결하게 할 수 있습니다. 이 간결함은 사람들에게는 좋지 않겠지만, 컴퓨터에게는 좋을 것입니다. 그 이유는 명령이 훨씬 짧아지고 더 효율적으로 실행할 수 있기 때문입니다. \n",
    "\n",
    "이전에 우리는 각 층에 2개의 노드만 갖는 2층 신경망에 대해 손으로 계산을 했습니다. 이것은 충분한 작업이었지만 각 층에 100개의 노드를 가진 5층 신경망에 대해 같은 계산을 한다고 상상해보세요. 필요한 계산식(각 층의 각 노드에 대해 합한 신호의 조합과 해당 가중치를 곱하는 것, 시그모이드 활성화 함수를 적용하는 것)을 작성하는 것만으로도 엄청난 작업이 될 것입니다. 이런 엄청난 작업을 위해 앞으로는 행렬을 사용할 것인데, 우리는 수학자들이 오래 전에 이미 행렬을 생각해 두었다는 사실에 참 놀라게 됩니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 행렬을 사용한 계산법\n",
    "\n",
    "행렬이 어떤 도움을 줄 수 있을까요? 두 가지 방법으로 도움을 줍니다.  \n",
    "- 첫 번째, 행렬은 이러한 모든 계산을 아주 간단하고 짧은 형식으로 압축하여 적을 수 있게 해줍니다. 우리 인간들에게는 아주 좋을 것입니다.  왜냐하면 많은 일을 하는 것은 지루하고, 오차를 범하기 쉽기 때문입니다. \n",
    "- 두 번째 이점은 많은 컴퓨터 프로그래밍 언어가 행렬 작업을 이해할 수 있으며, 실제 작업은 반복적이기 때문에 빠르고 효율적으로 인식할 수 있습니다.  요약하면, 행렬은 우리가 해야 하는 작업을 간결하고 쉽게 표현하며, 컴퓨터는 빠르고 효율적으로 계산을 이해할 수 있습니다.\n",
    "\n",
    "우리가 행렬 계산을 할 때 신경망에 쓰이는 의미 있는 단어를 사용한다면 어떻게 될지 다음 그림에서 살펴보세요. \n",
    "\n",
    "__**여기서 사용한 행렬의 각 요소 첨자가 수학적 표기법과 다른 것에 유의하길 바랍니다.**__\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} w_{11} & w_{21} \\cr w_{12} & w_{22} \\end{pmatrix} \n",
    "\\begin{pmatrix} x_1 \\cr x_2 \\end{pmatrix} = \n",
    "\\begin{pmatrix} w_{11} * x_1 + w_{21} * x_2 \\cr w_{12} * x_1 + w_{22} * x_2 \\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "__**여기서 사용한 행렬의 각 요소 첨자가 수학적 표기법과 다른 것에 유의하길 바랍니다.**__\n",
    "\n",
    "첫 번째 행렬은 두 개의 층의 노드 사이에 있는 가중치를 포함합니다. 두 번째 행렬은 첫 번째 입력층의 신호를 포함합니다. 이 두 행렬을 곱하여 얻은 결과는 두 번째 층의 노드에 들어가는 완화된 신호를 합한 것입니다. 잘 살펴보면, 여러분은 이것을 볼 수 있을 것입니다. 첫 번째 노드에서는 첫 번째 입력 $x_1$을 가중치 $w_{11}$로 조정한 후, 두 번째 입력 $x_2$을 가중치 $w_{21}$로 조정한 것과 더하였습니다. 이 값은 __순입력__이라고 하며, 순입력은 시그모이드 활성화 함수가 적용되기 전의 순입력 `z`값입니다. 다음 그림은 이것은 더 깔끔하게 보여줍니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model5.png?raw=true\" width=\"400\">\n",
    "<center>그림 9: 행렬을 이용한 신경망 출력 계산 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 유용합니다!\n",
    "\n",
    "왜일까요? \n",
    "\n",
    "두 번째 층의 각 노드에 들어가는 조정된 신호를 합한 값x를 위해 필요한 모든 계산은 행렬 곱셈을 사용하여 표현할 수 있기 때문입니다. 또한, 이는 다음과 같이 간결하게 표현할 수 있습니다.\n",
    "\n",
    "\\begin{align}\n",
    "Z = W • X\n",
    "\\end{align}\n",
    "\n",
    "여기서 $\\mathbf{W}$은 가중치 행렬이며, $\\mathbf{X}$은 입력 행렬이고, $\\mathbf{Z}$는 2번째 층에 들어온 순입력(행렬)입니다. 행렬은 종종 굵은 글씨체(볼드체)로 쓰여지는데, 그 이유는 하나의 숫자가 아닌 행렬이라는 것을 보여주기 위한 것입니다.\n",
    "\n",
    "우리는 이제 각 층에 몇 개의 노드가 있는지 신경 쓰지 않아도 됩니다. 더 많은 노드가 있다면, 행렬은 더 커질 것입니다. 하지만 우리는 더 길거나 더 크게 쓰지 않아도 됩니다. I에 2개의 요소가 있던지, 200개의 요소가 있던지 상관없이 우리는 간단하게 W•X라고 쓸 수 있습니다!  이제, 컴퓨터 프로그래밍 언어가 행렬 표기법을 이해할 수 있다면, 각 층과 각 노드에 대한 명령을 작성하지 않아도 $\\mathbf{Z = W • X}$을 계산하기 위한 많은 계산 작업을 할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 엄청납니다! 행렬 곱셈을 조금 이해하는 것만으로도 많은 노력을 들이지 않고도 신경망을 구현할 수 있는 강력한 도구를 갖게 되었습니다.\n",
    "\n",
    "자, 그러면 활성화 함수는 어떻게 할 수 있을까요? 이것은 쉽고, 행렬 곱셈도 필요 없습니다. 우리가 해야 할 것은 시그모이드 함수 $h(x) = \\frac{1}{1+e^{-x}}$를 행렬 $\\mathbf{Z}$의 각 요소에 적용하는 것뿐입니다. 너무 간단해 보이지만 여기서 다른 노드의 신호를 더하는 것이 아니기 때문에, 답은 이미 $Z$에 있습니다. 우리가 이전에 다루었던 바와 같이 활성화 함수는 단순히 임계값을 적용하여 그 결과를 생물학적 뉴런과 더 비슷하게 만듭니다. 따라서 두 번째 층의 마지막 출력은 다음과 같습니다.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\hat{y}} = A = sigmoid(Z)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\mathbf{A}$은 모든 노드의 출력을 표시할 때 사용하며, $\\mathbf{\\hat{y}}$는 신경망의 마지막 층 즉 __출력층__의 모든 출력을 표시할 때 사용합니다.\n",
    "\n",
    "일반적으로 신경망의 모든 노드의 입력과 출력의 관계식은 다음과 같이 표시할 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "Z &= W • A  \\\\\n",
    "A &= sigmoid(Z)\n",
    "\\end{align}\n",
    "\n",
    "다만, 입력층의 노드의 경우는 $A$가 입력 $X$와 같고, 출력층의 노드의 경우는 출력 $A$가 곧 $\\mathbf{\\hat{y}}$와 같습니다. \n",
    "\n",
    "\n",
    "은 첫 번째 층(입력층이라고 합니다.)과 다음 층 사이의 계산에 적용됩니다. 예를 들어 만약 3개의 층이 있다면, 두 번째 층의 출력을 세 번째 층의 입력으로 사용하는 대신, 가중치를 사용하여 완화한 결과를 더한 뒤, 행렬 곱셈을 다시 합니다.\n",
    "\n",
    "이론은 충분히 한 것 같습니다.  다음에는 행렬, 다차원 배열로 실제로 신경망의 연산을 해보겠습니다. 그 후에 신경망의 한 예시를 사용하여 기계학습의 핵심인 인공 신경망이 어떻게 동작하는지 살펴봅시다.  조금 더 크게 각각 3개의 노드를 가지고 있는 삼층신경망을 사용해 볼 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__요점:__\n",
    "- 신경망을 통해 신호를 처리하기 위해 필요한 많은 계산 과정은 행렬 곱셈으로 표현할 수 있습니다.\n",
    "- 행렬 곱셈으로 표현하는 것은 신경망의 크기에 상관 없이 우리가 간결하게 작성할 수 있도록 해줍니다.\n",
    "- 더 중요한 것은, 어떤 컴퓨터 프로그래밍 언어는 행렬 계산을 이해할 수 있으며, 이 계산이 아주 비슷하다는 것을 인지합니다. 이로부터 계산을 더 효율적이고 빠르게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 연습문제\n",
    "다음 신경망의 출력을 손으로 계산하고 또한 Python으로 계산하여 결과를 확인하십시오. 활성화 함수는 시그모이드 함수입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/2layer_model6.png?raw=true\" width=\"400\">\n",
    "<center>그림 10: 행렬을 이용한 신경망 출력 계산 연습 </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15 0.5 ]\n",
      "[0.75951092 0.62245933]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "x = np.array([1.0, 0.5])\n",
    "w = np.array([[0.9, 0.5], [0.1, 0.8]])\n",
    "z = np.dot(w, x)\n",
    "print(z)\n",
    "y = sigmoid(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리 \n",
    "    - 인공 신경망의 신호 전달\n",
    "    - 인공 신경망의 신호 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "_Rejoice always, pray continually, give thanks in all circumstances; for this is God’s will for you in Christ Jesus. (1 Thes 5:16-18)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
