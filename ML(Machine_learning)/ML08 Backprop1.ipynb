{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning For Everyone and Robot\n",
    "[Lecture notes](https://github.com/idebtor/HuStar-ML) for HuStar Project by idebtor@gmail.com, Handong Global University\n",
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 8 강: 역전파 1$^{Backpropagation}$ \n",
    "\n",
    "## 학습 목표\n",
    "    - 역전파의 원리를 이해한다\n",
    "\n",
    "## 학습 내용\n",
    "    - 출력층과 은닉층의 오차 구하기\n",
    "    - 비용 함수와 미분\n",
    "    - 역전파의 가중치 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 출력층과 은닉층의 오차 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 선형 분류기를 만들어 낼 때, 한 노드가 예측한 결과와 정답을 비교하여 차이가 난 부분 즉 오차를 사용하여 판별식을 개선할 수 있었습니다. 오차에 의하여 기울기를 얼마나 조정해야 하는지 결정하는 것은 비교적 간단하였습니다. 왜냐하면, 오차는 오직 하나의 노드에서 발생하였기 때문입니다. 그러나, 하나 이상의 여러 노드들이 결과를 만들어 내는데, 그 결과에 오차가 있을 때, 가중치를 어떻게 체계적으로 조정해야 오차를 줄일 수 있을까요? \n",
    "\n",
    "이 문제를 여기서 설명하고자 합니다. \n",
    "삼층 신경망의 순전파 과정의 일부분이 아래 그림과 같다고 합시다.  \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop1.png?raw=true\" width=\"400\">\n",
    "<center>그림 1: 출력 오차의 처리(1) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 보는 바와 같이 출력층 노드의 결과 $\\hat{y}_1 = 0.6$ 입니다. $\\hat{y}$은 모자를 쓴 것 같다고 하여, $y$햇$^{hat}$으로 읽으며, 이는 신경망이 순전파를 한 결과이며, 신경망의 예측값이라고 합니다. 밑첨자 1이 있으므로, 여러 예측값들 중에 첫 번째 값에 해당하며, 더 많은 예측값들이 있을 수 있습니다. \n",
    "\n",
    "햇이 없는 $y$ 값은 클래스 레이블$^{class \\ label}$이라고 부르며 우리가 목표로 하는 참 값에 해당합니다. 그러므로 출력 $\\hat{y}_1$과 클래스 레이블 $y_1$와 차이를 오차$e_1$이라고 정의하고, 또한 출력층의 오차는 $E_{output}$ 또는 $E^{[2]}$로 표기할 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    " E^{[2]} = e_1^{[2]}  &= y_1- \\hat{y}_1 \\\\\n",
    " 0.4 &= 1.0 - 0.6\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 노드에 영향을 주는 노드가 한 개만 있을 때에는 간단했지만, 두 개의 노드가 하나의 출력 노드에 영향을 미치고 있을 때, 출력층의 오차를 사용하여 어떻게 은닉층의 오차 $E_{hidden}$ 또는 $E^{[1]}$을 구할 수 있을까요? \n",
    "\n",
    "출력 노드가 오차를 내도록 영향을 미친 원인(가중치)들을 정확히 찾아내서 조금씩 개선하여 다시 계산해보려고 합니다. 한 방안은 오차가 나오도록 영향을 주는 모든 노드에 동등하게 오차를 나누어 배분하는 것도 한 가지 방안이 될 수 있습니다. 그림으로 표시하면 다음과 같습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop2.png?raw=true\" width=\"400\">\n",
    "<center>그림 2: 출력 오차의 처리(2) </center>\n",
    "\n",
    "수학적으로 행렬로 표시 한다면, 다음과 같이 표현할 수 있습니다. 여기서 $N$은 은닉층에서 출력노드로 입력되는 링크의 수 즉 은닉층의 노드의 수입니다. \n",
    "\n",
    "\n",
    "\\begin{align} \n",
    "E_{hidden} = \n",
    "\\begin{pmatrix} 1/N & 1/N & 1/N & ... \\cr 1/N & 1/N & 1/N & ... \\cr \n",
    "                1/N & 1/N & 1/N & ... \\cr ... & ... & ... & ... \\end{pmatrix} • E_{output}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 다른 방안은 오차를 동등하게 나누지 않는 것입니다. 대신에 더 큰 가중치를 갖는 연결에 더 큰 오차를 주는 것입니다. 왜 일까요? 이 연결은 오차에 더 많은 기여를 하기 때문입니다. 다음 그림은 이를 설명합니다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop3.png?raw=true\" width=\"400\">\n",
    "<center>그림 3: 출력 오차의 처리(3) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 노드에 들어가는 신호에 기여하는 노드는 두 개입니다. 가중치는 각각 $3.0$과 $1.0$입니다. 이 가중치와 비례하도록 오차를 나눈다면, 출력 오차의 $3/4$는 가장 큰 가중치를, $1/4$는 두 번째로 작은 가중치를 업데이트하기 위해 사용해야 합니다. 이를 가중치를 이용하여 수식으로 표현하면 다음과 같습니다. \n",
    "\n",
    "\\begin{align} \n",
    "\\frac{w_{11}}{w_{11} + w_{21}}  e_1^{[2]} &= \\frac{3.0}{3.0 + 1.0} 0.4 = 0.3 \\\\\n",
    "\\frac{w_{11}}{w_{11} + w_{21}}  e_1^{[2]} &= \\frac{1.0}{3.0 + 1.0} 0.4 = 0.1 \\\\\n",
    "E^{[1]} &= \\begin{pmatrix} 0.3 \\cr 0.1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "이와 같은 생각을 더 많은 노드로 확장 시켜봅시다. 출력 노드와 연결된 노드가 100개 있다면, 연결된 가중치의 크기에 따라 오차에 기여하는 정도와 비례$^{proportion}$하도록 출력 노드의 100개 연결만큼 오차를 쪼갤 것입니다.\n",
    "\n",
    "여기서 우리가 가중치를 두 가지 방법으로 사용하고 있다는 것을 볼 수 있을 것입니다. 첫째로 우리는 신경망의 입력층에서 출력층으로 신호를 전하는 순전파에 가중치를 사용합니다. 두번째로 우리는 출력에서 다시 신경망의 역방향으로 오차를 보내기 위해 가중치를 사용합니다. 이 방법을 __역전파$^{backpropagation}$__라 부르는 것은 당연할 것입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층이 2개의 노드를 가지고 있었다면, 우리는 두 번째 출력 노드에 같은 작업을 했을 것입니다. 두 번째 출력 노드는 자기 나름의 오차를 가지고 있을 것이므로, 연결마다 비슷하게 쪼개질 것입니다. \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop4.png?raw=true\" width=\"400\">\n",
    "<center>그림 4: 은닉층의 오차 구하기 </center>\n",
    "\n",
    "\n",
    "이것을 수학적으로 표현한 행렬은 다음과 같습니다. \n",
    "\n",
    "\\begin{align} \n",
    "E_{hidden} = \\begin{pmatrix} \n",
    "\\frac{w_{11}}{w_{11} + w_{21}} & \\frac{w_{12}}{w_{12} + w_{22}} \\cr\n",
    "\\frac{w_{21}}{w_{11} + w_{21}} & \\frac{w_{22}}{w_{12} + w_{22}} \n",
    "\\end{pmatrix} • E_{output}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "그런데, 문제는 이런 행렬은 우리가 이미 사용하고 있는 간단한 행렬로 계산할 수 없으며, 또한 우리가 사용하는 넘파이의 장점을 살릴 수 없습니다. \n",
    "\n",
    "만약에 오차를 배분하지 않고, 즉 분모가 같으니까 삭제버리고, 바로 가중치로만 곱해버리면 어떨까요? 가중치가 큰 뉴런이 출력층에 더 큰 오차를 만든다는 것을 직감적으로 알고 있으므로, 그 사실을 활용하여 훨씬 단순히 계산할 수 있습니다.   이것을 행렬로 표시하면 다음과 같이 간단합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{aligned}\n",
    "E_{hidden} &= W^{[2]T} • E_{output} \\\\\n",
    "E^{[1]} &= W^{[2]T} • E^{[2]}\n",
    "\\end{aligned}\n",
    "\n",
    "이 수식을 사용하면, 넘파이의 효율적인 행렬 연산을 바로 이용할 수 있습니다.  이 문제는 바로 다음 절에서 예제를 가지고 좀 더 자세히 다룰 것입니다.  기대하십시오.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두 출력 노드의 역전파 오차\n",
    "\n",
    "위 그림의 신경망은 두 개의 은닉층 노드와 두 개의 출력 노드가 있는 부분입니다.\n",
    "\n",
    "우리가 신경망을 학습시키지 않았을 때 두 출력 노드 모두 오차를 가질 가능성은 아주 높습니다. 두 오차 모두 신경망 내부의 가중치들을 얼마나 개선해야 하는지 알려주어야 한다는 것을 알 수 있습니다. 우리는 이전과 같은 방법, 즉 가중치와 비례하도록 연결 간에 출력 노드의 오차를 분배하는 방법을 사용할 수 있습니다.\n",
    "\n",
    "한 개 이상의 출력 노드를 있다고 해서 사실상 아무것도 달라지지 않습니다. 우리가 첫 번째 출력 노드에서 했던 것을 두 번째 출력 노드에서도 단순하게 반복하면 됩니다. 왜 이렇게 단순한 것일까요? 그 이유는 출력 노드로 들어가는 연결이 다른 출력 노드로 들어가는 연결이 서로 아무런 영향을 미치지 않기 때문입니다. 이 두 가지 연결 사이에는 의존성이 없다는 것입니다.  서로 독립적이죠.\n",
    "\n",
    "이 신경망을 다시 보면, 첫 번째 출력 노드의 오차를 $e_1$로 표기하였습니다. 오차는 참값을 의미하는 클래스 레이블 $y_1$과 신경망의 예측값 $\\hat{y}_1$ 차이라는 것을 기억하세요. 즉, $e_1 = (y_1 - \\hat{y}_1)$입니다. 두 번째 출력 노드의 오차는 $e_2$로 표기되었습니다. \n",
    "\n",
    "그림에서 오차 $e_1$는 가중치 $w_{11}$과 $w_{21}$을 가진 연결과 비례하여 분배되었다는 것을 볼 수 있습니다. 유사하게 $e_2$는 가중치 $w_{21}$과 $w_{22}$와 비례하여 분배되었습니다.  이렇게 분배한 것을 써보면서 확실히 알아봅시다. 오차 $e_1$은 두 가중치 $w_{11}$과 $w_{21}$을 개선하기 위해 사용되었습니다.  $w_{11}$을 업데이트하기 위해 사용된 $e_1$의 비율은 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "\\frac{w_{11}}{w_{11} + w_{21}} \n",
    "\\end{align}\n",
    "\n",
    "유사하게 $w_{21}$을 개선하기 위해 사용된 $e_1$의 비율은 다음과 같습니다.\n",
    "\n",
    "\\begin{align} \n",
    "\\frac{w_{21}}{w_{11} + w_{21}} \n",
    "\\end{align}\n",
    "\n",
    "이 수식은 조금 혼란스럽게 느껴질 수 있기 때문에 어떤 역할을 하는지 설명해봅시다. 알고 보면 이 수식에서는 오차 $e_1$을 큰 가중치에 더 많이 부여하고 작은 가중치에는 조금 부여한다는 간단한 개념을 나타냅니다. \n",
    "\n",
    "$w_{11} = 6$이고 $w_{21} = 3$으로 $w_{11}$이 $w_{21}$의 두 배라면, $w_{11}$을 업데이트하기 위해 사용된 $e_1$의 비율은 $\\frac{6}{(6+3)} = \\frac{6}{9} = \\frac{2}{3}$입니다. 더 작은 가중치 $w_{21}$에 대해서는 $e_1$의 $\\frac{1}{3}$만큼 남게 되며, 수식 $\\frac{3}{6+3} = \\frac{3}{9}$에서 $\\frac{1}{3}$라는 것을 이용하여 확인할 수 있습니다.\n",
    "\n",
    "여러분도 예상할 수 있듯이 가중치가 같다면, 절반으로 나누게 될 것입니다. 확실히 알아봅까요? $w_{11} = 4$이고 $w_{21} = 4$라고 한다면, 두 경우 모두 $\\frac{4}{4+4} = \\frac{4}{8} = \\frac{1}{2}$이 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 나아가기 전에 잠시 중단하고 우리가 한 것에 대해 다시 되돌아봅시다. 우리는 오차를 사용하여 신경망 내부의 어떤 매개변수, 즉 이 경우에는 가중치를 개선할 필요가 있다는 것을 알았습니다. 우리는 방금 신경망의 마지막 출력층으로 들어가는 신호를 완화하는 가중치들을 어떻게 해야 하는지 살펴보았습니다. 우리는 또한 한 개 이상의 출력 노드가 있을 때에도 더 복잡하지 않으며, 각 출력 노드에 똑같이 적용하면 된다는 것을 살펴보았습니다. 좋습니다!\n",
    "\n",
    "다음 질문은 2개 이상의 층이 있을 때 어떤 일이 벌어지는지에 대한 것입니다. 마지막 출력 증에서 역으로 가중치들을 업데이트하고자 할 때 어떻게 할 수 있을까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 두 층간의 역전파 오차\n",
    "\n",
    "다음 그림은 삼층 즉 입력층, 은닉층, 그리고 출력층을 가진 신경망을 보여줍니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop5.png?raw=true\" width=\"500\">\n",
    "<center>그림 5: 두 층간의 역전파 오차(1)</center>\n",
    "\n",
    "오른쪽의 마지막 출력층에서 되돌아보면, 출력층의 오차를 사용하여 마지막 층으로 들어가는 가중치를 개선하였다는 것을 알 수 있습니다. 출력 오차를 일반적으로 $E_{output}$ 혹은 $E^{[2]}$이라 표기하였으며, 은닉층과 출력층 사이의 연결된 가중치를 $W_{hidden\\_output}$, $W_{ho}$ 혹은 $W^{[2]}$라 하였습니다  각 연결과 연관된 특정 오차를 가중치의 크기와 비례하도록 분배하여 계산하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 시각화 하면 새로운 층이 있을 때 우리가 어떻게 해야 하는지 알 수 있습니다. 은닉층 노드의 출력과 연관된 오차 $E_{hidden}$을 $W_{ih}$와 비례하도록 입력층과 은닉층 사이의 연결에 분배합니다. 이러한 논리를 시각화하면 다음과 같습니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop6.png?raw=true\" width=\"500\">\n",
    "<center>그림 6: 두 층간의 역전파 오차(2)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 많은 층이 있었다면, 마지막 출력에서 후진하여 각 층에 같은 개념을 적용할 수 있습니다. 오차 정보의 흐름을 직관적으로 보더라도 우리가 왜 이것을 오차 역전파$^{backpropagation}$라 하는지 알 수 있을 것입니다.\n",
    "출력층 노드의 출력에서 나오는 오차 $E_{output}$를 처음 사용하였다면, 은닉층 노드에서는 어떤 오차를 사용해야 할까요? \n",
    "\n",
    "가운데 은닉층의 노드에는 명확한 오차가 없기 때문에 이는 좋은 질문입니다. 입력 신호를 내보낼 때에, 은닉층의 각 노드는 하나의 출력을 갖게 되었습니다. 여러분은 이것이 이 노드의 가중 합 입력에 적용된 활성화 함수라는 것을 기억할 것입니다. 하지만 오차를 어떻게 계산할 수 있을까요?\n",
    "\n",
    "은닉 노드에는 목표 출력이나 원하는 출력이 없습니다. 마지막 출력층 노드에 대한 목표값만 있을 뿐이며, 이는 학습 자료로부터 알 수 있습니다.  아이디어를 위해 위의 그림을 다시 살펴보세요! \n",
    "\n",
    "은닉층의 첫 번째 노드는 두 개의 출력층 노드와 연결되어 있습니다.  우리는 이전에 했던 바와 같이 출력 오차를 각 연결에 분배할 수 있다는 것을 알고 있습니다. 즉, 가운데 층 노드에서 나오는 두 개의 연결에 각각에 대해 오차가 있다는 것입니다. 이 두개의 연결 오차를 다시 합하여 두 번째 방법을 사용해 이 노드에 대한 오차를 만들 수 있습니다. 그 이유는 가운데 층 노드에 대한 목표값이 없기 때문입니다. 다음은 이 개념을 시각화 하여 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error1.png?raw=true\" width=\"500\">\n",
    "<center>그림 7: 은닉층으로의 오차 역전파(1) </center>\n",
    "\n",
    "여러분은 여기서 더 명확하게 알 수 있겠지만 확실하게 살펴봅시다.  은닉층에 대한 오차는 이전 층의 가중치를 업데이트 하기 위해 필요합니다. 이를 $E_{hidden}$이라 합니다. 하지만 우리는 정확한 정답을 가지고 있지 않습니다. 우리는 이 오차를 이 노드의 목표값과 출력값의 차이라고 할 수 없습니다. 그 이유는 학습 자료 표본이 마지막 출력 노드에 대한 목표값만 알려주기 때문입니다.\n",
    "\n",
    "학습 자료 표본은 마지막 노드의 출력이 무엇이어야 하는지를 알려줄 뿐입니다. 다른 층의 노드에서 나오는 출력이 어떻게 되어야 하는지는 알려주지 않습니다. 이것이 우리를 혼돈 시킵니다.  \n",
    "\n",
    "우리는 이전에 살펴보았던 오차 역전파를 사용하여 각 연결에 분배되었던 오차를 다시 합할 수 있습니다. 따라서 첫 번째 은닉 노드에 있는 오차는 같은 노드에서 이어진 모든 연결의 오차를 합한 것입니다. 위의 그림에서는 가중치 $w_{11}$을 가진 연결에서의 출력 오차의 일부분 $E_{output,1}$와 가중치 $w_{12}$을 가진 연결에서의 두 번째 출력 노드에서 나오는 출력 오차의 일부분 $E_{output,2}$을 보여줍니다. \n",
    "\n",
    "이를 수식으로 표기하면 다음과 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "e_{hidden,1} = e_1^{[1]} &= sum \\ of \\ split \\ errors \\ on \\ links \\ w_{11}^{[2]} \\ and \\ w_{12}^{[2]} \\\\\n",
    "                         &= e_{output,1} * \\frac{w_{11}}{w_{11} + w_{21}}\n",
    "                          + e_{output,2} * \\frac{w_{12}}{w_{12} + w_{22}}\n",
    "\\end{aligned}\n",
    "\n",
    "모든 이론을 실제로 보여주기 때문에 다음은 실제 숫자를 이용하여 3개의 층을 가진 간단한 신경망으로 역전파 되는 오차에 대해 설명합니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error2.png?raw=true\" width=\"500\">\n",
    "<center>그림 8: 은닉층으로의 오차 역전파(2)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 출력층 오차 $E^{[2]} = \\begin{pmatrix} 0.8 \\cr 0.5 \\end{pmatrix}$를 이용하여 은닉층 오차 $E^{[1]}$의 오차를 구하고자 합니다. 이 문제를 아래의 신경망에 나타내면, 출력층 오차를 비례 배분한 값을 빈칸에 먼저 답하고, 또한 은닉층의 오차의 물음표에 답을 해야 합니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error2.png?raw=true\" width=\"500\">\n",
    "<center>그림 9: 은닉층으로의 오차 역전파(3)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 출력층의 첫 번째 노드의 오차 즉 $e_2^{[1]} = 0.8$가 가중치 $2$와 $1$을 가지고 있는 링크에 각각 $\\frac{2}{2 + 3}$과 $\\frac{3}{2 + 3}$ 비례 배분되므로, $0.32$과 $0.48$ 입니다. \n",
    "\n",
    "- 출력층의 두 번째 노드의 오차 즉 $e_2^{[2]} = 0.5$가 가중치 $1$과 $4$를 가지고 있는 링크에 각각 $\\frac{1}{1 + 4}$과 $\\frac{4}{1 + 4}$ 비례 배분되므로, $0.1$과 $0.5$ 입니다. \n",
    "\n",
    "- 은닉층 첫 번째 노드의 오차는 출력층 두 개의 노드에서 각각 배분된 오차들$(0.32, 0.1)$을 합한 것이 되므로, $0.42$가 되며, 은닉틍 두 번째 노드의 오차 역시 두 오차들$(0.48, 0.4)$를 합하면, $0.88$ 입니다. 은닉층의 각 노드는 이 만큼 출력층의 오차를 내는데 공헌했다는 것입니다.  \n",
    "\n",
    "- $E_{hidden} = E^{[1]} = \\begin{pmatrix} e_1^{[1]}  \\cr e_2^{[1]}\\end{pmatrix} \n",
    "                        = \\begin{pmatrix} 0.32 + 0.1 \\cr 0.48 + 0.4 \\end{pmatrix} \n",
    "                        = \\begin{pmatrix} 0.42 \\cr 0.88 \\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 계산 결과를 신경망에 시각화하여 표시하면 다음과 같습니다.\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error4.png?raw=true\" width=\"500\">\n",
    "<center>그림 10: 은닉층으로의 오차 역전파(4)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리가 수행한 계산을 행렬로 표시해볼 수 있을까요?  좋은 연습이 될 것입니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "E_{hidden} = E^{[1]} &= \\begin{pmatrix} \\frac{w_{11}}{w_{11} + w_{21}} & \\frac{w_{12}}{w_{12} + w_{22}} \\cr\n",
    "             \\frac{w_{21}}{w_{11} + w_{21}} & \\frac{w_{22}}{w_{12} + w_{22}} \\end{pmatrix} • \n",
    "              \\begin{pmatrix} e_1  \\cr e_2 \\end{pmatrix} \\\\ \n",
    "           &= \\begin{pmatrix} \\frac{2}{2 + 3} & \\frac{1}{1 + 4} \\cr \\frac{3}{2 + 3} & \\frac{4}{1 + 4}  \\end{pmatrix}  • \\begin{pmatrix} 0.8 \\cr 0.5 \\end{pmatrix} \\\\\n",
    "           &= \\begin{pmatrix} 0.42 \\cr 0.88 \\end{pmatrix} \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 계산 방법을 기존의 가중치를 활용하기 위하여 간단히 계산하는 방법은 다음과 같습니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "E_{hidden} = E^{[1]} &= W^{[2]T} • E^{[2]} \\\\\n",
    "           &= \\begin{pmatrix} w_{11} & w_{12} \\cr w_{21} & w_{22} \\end{pmatrix} • \n",
    "              \\begin{pmatrix} e_1  \\cr e_2 \\end{pmatrix} \\\\ \n",
    "           &= \\begin{pmatrix} 2 & 1 \\cr 3 & 4  \\end{pmatrix}  • \\begin{pmatrix} 0.8 \\cr 0.5 \\end{pmatrix} \\\\\n",
    "           &= \\begin{pmatrix} 2.1 \\cr 4.4 \\end{pmatrix} \n",
    "\\end{aligned}\n",
    "\n",
    "두 개의 다른 방법으로 계산한 결과를 비교해보면 $E^{[2]}$ 오차가 비례 배분된 것은 결국 같다는 것을 알 수 있습니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "0.42:0.88 &= 0.477 \\\\\n",
    "0.21:4.44 &= 0.477\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이에 대한 자세한 의미는 바로 다음 강의에서 다루게 됩니다. \n",
    "\n",
    "__요점:__\n",
    "\n",
    "- 신경망은 서로 연결된 가중치들을 개선하면서 학습합니다. 이는 학습 자료로부터 주어진 목표값과 계산된 출력의 차이인 오차로 가능합니다.\n",
    "- 출력 노드의 오차는 단순히 원하는 오차와 실제 오차의 차이입니다.\n",
    "- 하지만 내부 노드와 연관된 오차는 명확하지 않습니다. 한 가지 방법은 연결된 가중치의 크기에 비례하여 출력층 오차를 분배하고 각 내부 노드에서 나눈 것을 다시 합하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__연습문제:__\n",
    "\n",
    "다음 그림은 출력층 오차를 역전파하여 은닉층(1)에 적용한 개념을 똑 같이 적용하여 은닉층(1)의 오차로부터 가상적으로 설정한 은닉층(0)의 오차를 구하십시오. 행렬을 사용하여 계산을 하고, 또한 주피터 노트북으로 결과를 확인하십시오. 위에서 제시한 두 가지 방법으로 계산하십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error5.png?raw=true\" width=\"500\">\n",
    "<center>그림 11: 오차 역전파 연습문제</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__해답:__\n",
    "\n",
    "우리의 계산 결과를 신경망에 시각화하여 표시하면 다음과 같습니다. \n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/backprop_error6.png?raw=true\" width=\"500\">\n",
    "<center>그림 12: 오차 역전파 연습문제 해답</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 행렬을 이용한 역전파\n",
    "\n",
    "힘든 계산을 간소화하기 위해 행렬 곱셈을 사용할 수 있을까요? 행렬 곱셈은 이전에 입력 신호를 전달하기 위한 많은 계산을 할 때 도움이 되었습니다.  행렬 곱셈을 사용하여 더 간결한 오차 역전파를 만들 수 있는지 알아보기 위해 기호를 사용하여 한 단계씩 적어 봅시다. 덧붙여서, 이것은 과정을 __벡터화$^{vectorize}$__ 한다고 합니다.\n",
    "\n",
    "행렬 형식으로 많은 계산을 표현하는 것은 우리가 더 간결하게 적을 수 있도록 하며, 컴퓨터가 더 효율적으로 일을 처리하도록 합니다. 그 이유는 해야 하는 계산 중에서 비슷하게 반복되는 것을 이용할 수 있기 때문입니다. \n",
    "\n",
    "처음에는 신경망의 마지막 출력층에서 나오는 오차에서부터 시작합니다. 출력층에는 두 개의 노드만 있기 때문에 $e_1$과 $e_2$라 합니다. \n",
    "\n",
    "\\begin{align}\n",
    "E_{output} &= \\begin{pmatrix} e_1  \\cr e_2 \\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "따라서, 이 신경망이 삼층으로 구성되어 있다면, 출력층 오차는 층의 정보를 포함하여 다음과 같이 나타낼 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "E^{[2]} &= \\begin{pmatrix} e_1^{[2]}  \\cr e_2^{[2]} \\end{pmatrix} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 은닉층 오차에 대한 행렬을 만듭니다. 어려울 것으로 생각되기 때문에 조금씩 해봅시다. 먼저, 은닉층의 첫 번째 노드를 이용해 봅시다. 앞에서 다루었던 신경망을 다시 보면, 첫 번째 은닉 노드의 오차는 출력층에서 이어진 두 개의 길을 가지고 있다는 것을 알 수 있습니다. 이 길을 따라가면 오차 신호는 $e_1 * \\frac{w_{11}}{w_{11} + w_{21}}$과 $e_2 * \\frac{w_{12}}{w_{12}+w_{22}}$입니다. 이제 두 번째 은닉층을 살펴보면 오차로 이어지는 두 개의 길을 볼 수 있으며, $e_1 * \\frac{w_{21}}{w_{21} + w_{11}}$과 $e_2 * \\frac{w_{22}}{w_{22}+w_{12}}$입니다. 우리는 이전에 이미 이 수식이 어떻게 계산되는지 살펴보았습니다.\n",
    "\n",
    "따라서 은닉층에 대해 다음과 같이 행렬로 나타냅니다. 우리가 바라는 것 보다 조금 더 복잡합니다.\n",
    "\n",
    "\\begin{aligned} \n",
    "E_{hidden} = \\begin{pmatrix} \\frac{w_{11}}{w_{11} + w_{21}} & \\frac{w_{12}}{w_{12} + w_{22}} \\cr\n",
    "             \\frac{w_{21}}{w_{11} + w_{21}} & \\frac{w_{22}}{w_{12} + w_{22}} \\end{pmatrix} • \n",
    "             \\begin{pmatrix} e_1  \\cr e_2 \\end{pmatrix} \\\\ \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 이미 알고 있는 것을 사용하여 이것을 아주 간단한 행렬의 곱셈으로 다시 작성하면 더 좋을 것 같습니다. 우리는 가중치, 전송 신호, 그리고 출력 오차 행렬을 갖고 있습니다. 이를 할 수 있다면 우리에게 큰 도움이 될 것입니다.\n",
    "\n",
    "불행히도 이전에 신호를 앞으로 전송할 때 했던 것과 같이 아주 간단한 행렬 곱셈으로 바꾸지 못합니다. 위에 보이는 크고 복잡한 행렬의 분수는 풀기 어렵습니다! 우리가 이 복잡한 행렬을 계산 가능한 행렬의 간단한 조합으로 깔끔하게 분리할 수 있다면 도움이 될 것입니다.\n",
    "\n",
    "무엇을 할 수 있을까요? 우리는 계산을 효율적으로 하기 위해 행렬 곱셈을 사용하고자 합니다.  이제 이것으로 무슨 일이 일어나는지 잘 살펴보세요!\n",
    "\n",
    "위의 수식을 다시 살펴보세요. 가장 중요한 것은 연결된 가중치 $w_{ij}$를 가진 출력 오차 $e_n$을 곱하는 것임을 알 수 있습니다.  가중치가 클수록 더 많은 출력 오차가 은닉층으로 이동됩니다. 이는 중요한 사실입니다. 분모의 값은 값들을 사실상 정규화$^{normalize}$ 하기 위해 필요한 것입니다. 그러므로, 만약 우리가 이 분모를 무시한다고 할지라도, 역전파되는 오차가 비율로 크기$^{scaling}$ 만 좀 다를 뿐입니다. 즉, 다른 말로 분모를 제외한다면, $e_1 * \\frac{w_{11}}{w_{11} + w_{21}}$을 $e_1 * w_{11}$ 로 더 간단하게 표현할 수 있게 됩니다.\n",
    "\n",
    "\\begin{aligned} \n",
    "e_1 * \\frac{w_{11}}{w_{11} + w_{21}} \\ \\longrightarrow \\ e_1 * w_{11}\n",
    "\\end{aligned}\n",
    "\n",
    "이것을 마쳤다면, 행렬 곱셈은 다음과 같이 더 간단히 나타낼 수 있습니다. 놀랍죠? 한 가지 더 있습니다. \n",
    "\n",
    "\\begin{aligned} \n",
    "E_{hidden} = \\begin{pmatrix} w_{11} & w_{12} \\cr\n",
    "                             w_{21} & w_{22} \\end{pmatrix} • \n",
    "             \\begin{pmatrix} e_1  \\cr e_2 \\end{pmatrix} \\\\ \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 그런데, 이 가중치 행렬은 우리가 이전에 만들었던 것과 비슷하지만 같지 않습니다. </span> 그 내용이 이전 것과 대각선으로 반전된 모양이기 때문에 오른쪽 위는 왼쪽 아래에 있고, 왼쪽 아래는 오른쪽 위에 있습니다. 이는 행렬 __전치$^{transpose}$__ 라 하며, $W^T$로 표기합니다.\n",
    "\n",
    "다음과 같이 숫자 행렬을 전치 시키는 예제를 통해 어떻게 되는지 확실히 알 수 있습니다. 행렬의 행과 열의 개수가 다르더라도 동일하게 적용됩니다\n",
    "\n",
    "\\begin{aligned} \n",
    "\\begin{pmatrix} 1 & 2 \\cr 3 & 4 \\end{pmatrix}^T = \n",
    "\\begin{pmatrix} 1 & 3 \\cr 2 & 4 \\end{pmatrix}\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned} \n",
    "\\begin{pmatrix} 1 & 2 \\cr 3 & 4 \\cr 5 & 6 \\end{pmatrix}^T = \n",
    "\\begin{pmatrix} 1 & 3 & 5 \\cr 2 & 4 & 6 \\end{pmatrix}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리가 원했던 것처럼 오차를 역전파하는 행렬로 구하는 공식을 구했습니다.\n",
    "<span style=\"color:blue\">\n",
    "\\begin{aligned} \n",
    "E_{hidden} &= W^T_{hidden\\_output} • E_{output} \\\\ \n",
    "E^{[1]} &= W^{[2]T} • E^{[2]}\n",
    "\\end{aligned}\n",
    "</span>\n",
    "하지만 정규화$^{normalize}$ 하는 부분을 삭제해 버렸는데 괜찮을까요? \n",
    "\n",
    "이와 같이 더 간단한 오차 신호 조정은 우리가 이전에 계산했던 정교한 것만큼 잘 동작한다는 것이 이미 여러 방법으로 판명되었습니다. 그러므로, 더 간단한 방법이 아주 잘 동작한다면, 그것을 사용합시다!\n",
    "\n",
    "이에 대해 더 많이 생각해보고 싶다면, 아주 크거나 작은 오차가 역전파된다 하더라도 신경망은 다음에 반복되는 학습 과정에서 스스로 개선을 할 것이라는 것을 알 수 있습니다. 중요한 것은 역전파되는 오차가 연결된 가중치의 크기로부터 많은 것을 알 수 있기 때문에 이를 고려한다는 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 아주 많은 공부를 했습니다. 그리고 그 공부는 신경망의 핵심적인 공부였습니다!\n",
    "\n",
    "__요점:__\n",
    "- 오차를 역전파하는 것은 행렬 곱셈으로 표현할 수 있다.\n",
    "- 이는 신경망의 크기와 관계 없이 간결하게 표현할 수 있도록 해주며, 행렬 계산을 이해하는 컴퓨터 언어가 더 효율적이고 빠르게 일을 처리할 수 있다.\n",
    "- 이는 신호를 앞으로 전파하는 것과 오차를 역전파하는 것을 모두 행렬을 이용하여 효율적으로 할 수 있다. \n",
    "\n",
    "우리가 마지막으로 다루어야 할 얼마 남지 않은 이론 강의들은 아주 흥미롭지만 맑은 머리를 필요로 하기 때문에 충분한 휴식을 취하도록 하세요.  여러분은 어떻게 휴식을 취하나요?  복잡한 생각을 했다면, 휴식하는 중에 단순한 생각을 하는 것이 좋겠습니다.  \n",
    "\n",
    "요즘 말로 '멍 때린다'고 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리 \n",
    "    - 인공 신경망의 신호 전달\n",
    "    - 인공 신경망의 신호 처리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
